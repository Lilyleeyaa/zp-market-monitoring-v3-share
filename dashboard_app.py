import streamlit as st
import pandas as pd
import datetime

# Updated: 2026-01-09 (Final Fix for English Mode)

# Page configuration
st.set_page_config(
    page_title="Health Market Monitor",
    page_icon="ğŸ¥",
    layout="wide"
)

# Apply Noto Sans KR font globally
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');

* {
    font-family: 'Noto Sans KR', sans-serif !important;
}

html, body, div, span, p, h1, h2, h3, h4, h5, h6 {
    font-family: 'Noto Sans KR', sans-serif !important;
}

.stMarkdown, .stText, .stButton button, .stSelectbox, .stMultiSelect {
    font-family: 'Noto Sans KR', sans-serif !important;
}

/* Apply to text areas and inputs */
textarea, input, .stTextArea textarea, .stTextInput input {
    font-family: 'Noto Sans KR', sans-serif !important;
}

/* Streamlit specific */
[data-testid="stMarkdownContainer"] {
    font-family: 'Noto Sans KR', sans-serif !important;
}
</style>
""", unsafe_allow_html=True)

# Title
st.title("ğŸ¥ Healthcare Market Monitoring")
st.markdown("Automated news monitoring & analysis system")

import glob
import os

# Load data
@st.cache_data
def load_data():
    try:
        base_dir = "data/articles_raw"
        if not os.path.exists(base_dir):
            base_dir = "../data/articles_raw"
            
        # Priority 1: Ranked files
        ranked_files = glob.glob(os.path.join(base_dir, "articles_ranked_*.csv"))
        # Priority 2: Raw files
        raw_files = glob.glob(os.path.join(base_dir, "articles_*.csv"))
        
        target_file = None
        file_type = "None"
        
        if ranked_files:
            target_file = max(ranked_files, key=os.path.getctime)
            file_type = "AI Ranked"
        elif raw_files:
            target_file = max(raw_files, key=os.path.getctime)
            file_type = "Raw Data"
            
        if not target_file:
            return pd.DataFrame(), None, None
            
        df = pd.read_csv(target_file)
        
        # Convert date column
        try:
            df['published_date'] = pd.to_datetime(df['published_date']).dt.date
        except:
            pass
            
        # Ensure category column exists
        if 'category' not in df.columns:
            df['category'] = 'General'
            
        # Ensure keywords column exists (for filtering)
        if 'keywords' not in df.columns:
            df['keywords'] = 'General'
            
        return df, os.path.basename(target_file), file_type
        
    except Exception as e:
        return pd.DataFrame(), None, str(e)

df, filename, file_type = load_data()

if filename:
    if file_type == "AI Ranked":
         st.toast(f"Loaded: {filename} (AI Ranked)", icon="ğŸ¤–")
    else:
         st.toast(f"Loaded: {filename} (Raw Data)", icon="ğŸ“‚")
elif file_type and "None" not in str(file_type): # Error case
    st.error(f"Error loading data: {file_type}")

if df.empty:
    st.warning("No data found. Please run the crawler first.")
    st.stop()


# Main Layout
st.markdown("""
<style>
    /* Global Background & Font */
    .stApp {
        background-color: #F0F8F8; /* Very Light Teal/Grey */
    }
    
    /* Header/Title */
    h1 {
        color: #006666 !important; /* Deep Teal */
    }
    
    /* Article Card Styles */
    .article-card {
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 15px;
        background-color: #ffffff; /* White card */
        border-left: 5px solid #0ABAB5; /* Tiffany Blue Accent */
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    .article-title {
        font-size: 18px;
        font-weight: bold;
        color: #008080; /* Teal */
        text-decoration: none;
    }
    .article-title:hover {
        color: #0ABAB5; /* Tiffany Blue on Hover */
        text_decoration: underline;
    }
    
    .article-meta {
        font-size: 12px;
        color: #888;
    }
    
    .category-badge {
        background-color: #E0F2F1; /* Light Teal background */
        color: #00695C; /* Dark Teal text */
        padding: 4px 8px;
        border-radius: 12px;
        font-size: 12px;
        font-weight: 500;
        margin-left: 5px;
    }

    .article-summary {
        font-size: 14px;
        color: #444;
        margin-top: 8px;
        line-height: 1.6;
    }
    
    /* Button Styles */
    .stButton>button {
        background-color: #0ABAB5 !important;
        color: white !important;
        border: none;
    }
</style>
""", unsafe_allow_html=True)

# Custom Glossary for Pre-translation check
# (Applied BEFORE Google Translate to ensure correct terminology)
EXTRA_GLOSSARY = {
    "ë°ì¼ë¦¬íŒœ": "Daily Pharm",
    "ì•½ì‚¬ê³µë¡ ": "Yaksagongron",
    "ë©”ë””íŒŒë‚˜": "Medipana",
    "ì˜í•™ì‹ ë¬¸": "Medical Times",
    "ì²­ë…„ì˜ì‚¬": "Doctor's News",
    "ë‰´ìŠ¤1": "News1",
    "ë‰´ì‹œìŠ¤": "Newsis",
    
    # Industry Terms
    "ì²˜ë°©ê¶Œ ì§„ì…": "Entry into Prescription Market",
    "ì²˜ë°©ê¶Œ": "Prescription Market",
    "ê¸‰ì—¬ í™•ëŒ€": "Reimbursement Expansion",
    "ê¸‰ì—¬": "Reimbursement",
    "ë¹„ê¸‰ì—¬": "Non-Reimbursement",
    "ì•½ê°€ ì¸í•˜": "Price Cut",
    "ì•½ê°€": "Drug Price",
    "ì œë„¤ë¦­": "Generic",
    "ì˜¤ë¦¬ì§€ë„": "Original",
    "í’ˆì ˆ": "Out of Stock",
    "ê³µê¸‰ë¶€ì¡±": "Supply Shortage",
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption",
    "ì„ìƒ": "Clinical Trial",
    "í—ˆê°€": "Approval",
    "ì‹ì•½ì²˜": "MFDS",
    "ì‹¬í‰ì›": "HIRA",
    "ê±´ë³´ê³µë‹¨": "NHIS",
    "ì•±ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ì—¡ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ìƒê¸‰ì¢…í•©ë³‘ì›": "Tertiary General Hospital",
    "ê±´ê¸°ì‹": "Health Functional Food",
    "í”„ë¦¬í•„ë“œ": "Pre-filled",
}

# Helper function for translation (Global Scope)
import requests
import json
import sys

# Force UTF-8 encoding for Windows console (just in case)
# sys.stdout.reconfigure(encoding='utf-8')

# Configure Gemini API (Direct REST API for Python 3.8 compatibility)
GENAI_API_KEY = "AIzaSyD5HUixHFDEeifmY5NhJCnL4cLlxOp7fp0"
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GENAI_API_KEY}"

@st.cache_data(show_spinner=False)
def translate_text(text, target='en'):
    if not text: return ""
    
    # 1. Try Gemini API first (High Quality) with Retry Logic
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Construct explicit prompt with glossary context
            full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
            glossary_context = "\n".join([f"- {k}: {v}" for k, v in full_glossary.items()])
            
            prompt = f"""
            You are a professional medical translator for pharmaceutical business news.
            Translate the following Korean text to English.
            
            Guidelines:
            1. **Medical Context**: Interpret ambiguous phonetic terms (Konglish) using standard medical terminology (e.g., 'í”„ë¦¬í•„ë“œ' -> 'Pre-filled', NOT 'Free-filled').
            2. **Industry Standards**: Use formal business language ('Entering market', 'Conclusion of contract').
            3. **Glossary Adherence**: You MUST strictly use the glossary below:
            {glossary_context}
            
            Text to translate:
            "{text}"
            
            Output only the translated English text, no explanations.
            """
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }]
            }
            
            headers = {'Content-Type': 'application/json'}
            response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload), timeout=5)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    return result['candidates'][0]['content']['parts'][0]['text'].strip()
            elif response.status_code == 429:
                if attempt < max_retries - 1:
                    time.sleep(2) # Wait 2s before retry
                    continue
            else:
                print(f"[Gemini API Error] {response.status_code}: {response.text}")
                break 
                
        except Exception as e:
            print(f"[Gemini Exception] {e}")
            break
        
    # Check if we should retry (Simple retry logic wrapper)
    # Since we are inside the function, we can just loop. 
    # But to minimize diff, let's wrap the logic above in a loop.
    pass

@st.cache_data(show_spinner=False)
def translate_article_batch(title, summary, keywords):
    """
    Translates Title, Summary, and Keywords in a single API call to reduce latency.
    Returns: (translated_title, translated_summary, translated_keywords)
    """
    if not title and not summary: return title, summary, keywords

    # Construct combined text
    combined_text = f"Title: {title}\nSummary: {summary}\nKeywords: {keywords}"
    
    # 1. Try Gemini Combined Call
    result_text = translate_text(combined_text)
    
    # 2. Parse the result (Simple heuristic parsing)
    # Gemini usually returns the format requested, but we need to be robust.
    # Since translate_text returns a string, we hope it preserves the structure.
    # Let's verify if we need to adjust the prompt in translate_text.
    # Actually, reusing translate_text is risky if the prompt expects 'Ko -> En'.
    # It might treat "Title: ..." as part of the sentence to translate.
    # So we need a specialized implementation or just trust translate_text handles newlines well.
    
    # BETTER APPROACH: Just use translate_text logic but with a specific prompt structure?
    # No, translate_text has a fixed prompt.
    # Let's modify translate_text to be more generic OR clearer.
    # The current translate_text prompt says "Translate the following Korean text...".
    # If we pass structured text, it essentially translates the Values. 
    # Let's parse the output.
    
    t_title, t_summary, t_keywords = title, summary, keywords
    
    try:
        lines = result_text.split('\n')
        for line in lines:
            if line.startswith("Title:") or line.startswith("Title :"):
                t_title = line.split(":", 1)[1].strip()
            elif line.startswith("Summary:") or line.startswith("Summary :"):
                t_summary = line.split(":", 1)[1].strip()
            elif line.startswith("Keywords:") or line.startswith("Keywords :"):
                t_keywords = line.split(":", 1)[1].strip()
    except:
        pass # Fallback to original if parsing fails
        
    return t_title, t_summary, t_keywords

# Helper function for translation (Global Scope)
import requests
import json
import time

# Configure Gemini API (Direct REST API for Python 3.8 compatibility)
GENAI_API_KEY = "AIzaSyD5HUixHFDEeifmY5NhJCnL4cLlxOp7fp0"
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GENAI_API_KEY}"

def translate_text(text, target='en'):
    if not text: return ""
    
    # 1. Try Gemini API first (High Quality) with Retry Logic
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Construct explicit prompt with glossary context
            full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
            glossary_context = "\n".join([f"- {k}: {v}" for k, v in full_glossary.items()])
            
            prompt = f"""
            You are a professional pharmaceutical translator. 
            Translate the following Korean text to English.
            
            Rules:
            1. Maintain professional industry terminology.
            2. Use the specific glossary below for strict term matching:
            {glossary_context}
            
            Text to translate:
            "{text}"
            
            Output only the translated English text, no explanations.
            """
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }]
            }
            
            headers = {'Content-Type': 'application/json'}
            response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload), timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    return result['candidates'][0]['content']['parts'][0]['text'].strip()
            elif response.status_code == 429:
                if attempt < max_retries - 1:
                    time.sleep(2) # Wait 2s before retry
                    continue
            else:
                print(f"[Gemini API Error] {response.status_code}: {response.text}")
                break # Don't retry on 400/500 errors usually (unless 503)
                
        except Exception as e:
            print(f"[Gemini Exception] {e}")
            break
            
    # 2. Fallback to Google Translator + Pre-processing (If Gemini fails)
    try:
        # Pre-translate Glossary Terms manually
        full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
        sorted_terms = sorted(full_glossary.keys(), key=len, reverse=True)
        
        processed_text = text
        for kr_term in sorted_terms:
            if kr_term in processed_text:
                processed_text = processed_text.replace(kr_term, full_glossary[kr_term])
                
        from deep_translator import GoogleTranslator
        # Force source='ko' to ensure it translates the remaining Korean parts even with English glossary terms mixed in
        return GoogleTranslator(source='ko', target=target).translate(processed_text)
    except Exception as e:
        st.toast(f"Translation Error: {e}", icon="âš ï¸")
        return text

# Keyword EN/KR Mapping for Filter
KEYWORD_MAPPING = {
    # Distribution
    "ì˜ì•½í’ˆìœ í†µ": "Pharmaceutical Distribution",
    "ì§€ì˜¤ì˜": "GeoYoung",
    "DKSH": "DKSH",
    "ë¸”ë£¨ì— í…": "BlueMtech",
    "ë°”ë¡œíŒœ": "Baropharm",
    "ìš©ë§ˆ": "Yongma",
    "ì‰¥ì»¤": "Schenker",
    "DHL": "DHL",
    "LXíŒí† ìŠ¤": "LX Pantos",
    "CJ": "CJ",
    
    # BD
    "ê³µë™íŒë§¤": "Co-Promotion",
    "ì½”í”„ë¡œëª¨ì…˜": "Co-Promotion",
    "ìœ í†µê³„ì•½": "Distribution Agreement",
    "íŒê¶Œ": "Sales Rights",
    "ë¼ì´ì„ ìŠ¤": "License",
    "M&A": "M&A",
    "ì¸ìˆ˜": "Acquisition",
    "í•©ë³‘": "Merger",
    "ì œíœ´": "Partnership",
    "íŒŒíŠ¸ë„ˆì‹­": "Partnership",
    "ê³„ì•½": "Contract",
    "ìƒë¬¼í•™ì ì œì œ": "Biologics",
    "ì½œë“œì²´ì¸": "Cold Chain",
    "CSO": "CSO",
    "íŒì´‰ì˜ì—…ì": "Sales Agent",
    "ì œë„¤ë¦­": "Generic",
    "íŠ¹í—ˆë§Œë£Œ": "Patent Expiry",
    "êµ­ê°€ë°±ì‹ ": "National Vaccine",
    "ë°±ì‹ ": "Vaccine",
    
    # Approval
    "í—ˆê°€": "Approval",
    "ì‹ ì œí’ˆ": "New Product",
    "ì¶œì‹œ": "Launch",
    "ì‹ ì•½": "New Drug",
    "ì ì‘ì¦": "Indication",
    "ì œí˜•": "Formulation",
    "ìš©ëŸ‰": "Dosage",
    
    # Reimbursement
    "ë³´í—˜ë“±ì¬": "Reimbursement",
    "ê¸‰ì—¬": "NHI Coverage",
    "ì•½ê°€": "Drug Price",
    
    # Zuellig
    "ì¥´ë¦­": "Zuellig",
    "ì§€í”¼í…Œë¼í“¨í‹±ìŠ¤": "ZP Therapeutics",
    "ë¼ë¯¸ì‹¤": "Lamisil",
    "ì•¡í‹°ë„˜": "Actinum",
    "ë² íƒ€ë”˜": "Betadine",
    "ì‚¬ì´í´ë¡œì œìŠ¤íŠ¸": "Cyclogest",
    "ë¦¬ë¸Œíƒ€ìš”": "Libtayo",
    
    # Client
    "í•œë…": "Handok",
    "MSD": "MSD",
    "ì˜¤ê°€ë…¼": "Organon",
    "í™”ì´ì": "Pfizer",
    "ì‚¬ë…¸í”¼": "Sanofi",
    "ì•”ì  ": "Amgen",
    "GSK": "GSK",
    "ë¡œìŠˆ": "Roche",
    "ë¦´ë¦¬": "Lilly",
    "ë…¸ë°”í‹°ìŠ¤": "Novartis",
    "ë…¸ë³´ë…¸ë””ìŠ¤í¬": "Novo Nordisk",
    "ë¨¸í¬": "Merck",
    "ë ˆì½”ë¥´ë‹¤í‹°": "Recordati",
    "ì…€ì§„": "Celgene",
    "í…Œë°”í•œë…": "Teva-Handok",
    "ë² ë§ê±°ì¸ê²”í•˜ì„": "Boehringer Ingelheim",
    "BMS": "BMS",
    "ì•„ìŠ¤íŠ¸ë¼ì œë„¤ì¹´": "AstraZeneca",
    "ì• ë¸Œë¹„": "AbbVie",
    "íŒŒë§ˆë…¸ë¹„ì•„": "Pharmanovia",
    "ë¦¬ì œë„¤ë¡ ": "Regeneron",
    "ë°”ì´ì—˜": "Bayer",
    "ì•„ìŠ¤í…”ë¼ìŠ¤": "Astellas",
    "ì–€ì„¼": "Janssen",
    "ë°”ì´ì˜¤ì  ": "Biogen",
    "ì…ì„¼": "Ipsen",
    "ì• ë³´íŠ¸": "Abbott",
    "ì•ˆí…ì§„": "Antengene",
    "ë² ì´ì§„": "BeiGene",
    "ì…€íŠ¸ë¦¬ì˜¨": "Celltrion",
    "í—¤ì¼ë¦¬ì˜¨": "Haelion",
    "ì˜¤í ë¼": "Opella",
    "ì¼„ë·°": "Kenvue",
    "ë¡œë ˆì•Œ": "L'Oreal",
    "ë©”ë‚˜ë¦¬ë‹ˆ": "Menarini",
    "ìœ„ê³ ë¹„": "Wegovy",
    "ë§ˆìš´ìë¡œ": "Mounjaro",
    
    # Therapeutic
    "ë‚œì„": "Infertility",
    "ë¶ˆì„": "Infertility",
    "í•­ì•”ì œ": "Anticancer",
    
    # Supply
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption",
    "ê³µê¸‰ë¶€ì¡±": "Supply Shortage",
    "í’ˆì ˆ": "Out of Stock",
    "í’ˆê·€": "Shortage",
}

# Top Control Bar (Language & Filters)
# Use a clearer layout
st.markdown("### ğŸ” Filters & Settings")

f_col1, f_col2, f_col3, f_col4, f_col5, f_col6 = st.columns([1.5, 2, 2, 2, 2, 1.5])

with f_col1:
    # 1. Language Selector (Filter Style)
    lang_opt = st.selectbox("ğŸŒ Language", ["Korean", "English"], index=0)
    use_english = (lang_opt == "English")

with f_col2:
    # 2. Date Filter
    min_date = df['published_date'].min()
    max_date = df['published_date'].max()
    date_range = st.date_input("ğŸ“… Date Range", [min_date, max_date])
    if len(date_range) == 2:
        start_date, end_date = date_range
    else:
        start_date, end_date = min_date, max_date

with f_col3:
    # 3. Category Filter
    all_categories = sorted(df['category'].dropna().unique().tolist())
    selected_categories = st.multiselect("ğŸ“‚ Category", all_categories, default=[]) # Default empty = All
    if not selected_categories: 
         selected_categories = all_categories

# --- Logic for Dynamic Keyword Filter ---
# Filter data by Date & Category FIRST to determine available keywords
temp_mask = (
    (df['published_date'] >= start_date) & 
    (df['published_date'] <= end_date) &
    (df['category'].isin(selected_categories))
)
df_filtered_step1 = df[temp_mask]

with f_col4:
    # 4. Keyword Filter (Dynamic Options)
    available_keywords = []
    if 'keywords' in df_filtered_step1.columns:
        # Assuming keywords column might have multiple values?
        # If it's single value per row:
        available_keywords = sorted(df_filtered_step1['keywords'].dropna().unique().tolist())
    
    # Translate keyword options if English mode
    if use_english:
        keyword_options = [KEYWORD_MAPPING.get(k, k) for k in available_keywords]
        # Create reverse mapping for selection
        en_to_kr = {KEYWORD_MAPPING.get(k, k): k for k in available_keywords}
    else:
        keyword_options = available_keywords
    
    selected_keywords_display = st.multiselect("ğŸ”‘ Keyword", keyword_options, default=[])
    
    # Convert back to Korean for filtering (data is in Korean)
    if use_english:
        selected_keywords = [en_to_kr.get(k, k) for k in selected_keywords_display]
    else:
        selected_keywords = selected_keywords_display


with f_col5:
    # 5. Sort Option
    sort_opts = ["AI Relevance", "Latest Date", "Category", "Keyword"]
    sort_mode = st.selectbox("ğŸ“Š Sort By", sort_opts)

with f_col6:
    # 6. AI Recommended Filter
    show_ai_only = st.checkbox("ğŸ¤– AI Only", value=True, help="Show only AI recommended articles (final_score >= 0.5)")


# --- Apply Final Filter ---
mask = temp_mask # Start with date/cat mask
if selected_keywords:
    mask = mask & (df['keywords'].isin(selected_keywords))

# Apply AI filter if checkbox is enabled
if show_ai_only and 'lgbm_score' in df.columns:
    # VIP Keywords (critical companies/topics)
    VIP_KEYWORDS = [
        'DKSH', 'GSK', 'MSD', 'ê³µë™íŒë§¤', 'ë…¸ë°”í‹°ìŠ¤', 'ë…¸ë³´ë…¸ë””ìŠ¤í¬',
        'ë¼ë¯¸ì‹¤', 'ë¡œìŠˆ', 'ë¦´ë¦¬', 'ë¸”ë£¨ì— í…', 'ì‚¬ë…¸í”¼', 'ì•”ì  ', 'ì˜¤ê°€ë…¼',
        'ìœ„ê³ ë¹„', 'ì¥´ë¦­', 'ì§€ì˜¤ì˜', 'ì½”í”„ë¡œëª¨ì…˜', 'íŠ¹í—ˆë§Œë£Œ', 'í•œë…', 'í™”ì´ì',
        'ë©”ë‚˜ë¦¬ë‹ˆ'
    ]
    
    # 1. LGBM Top 20 (Pure AI discovery)
    # SAFE THRESHOLD: lgbm_score >= 0.18 (Slightly relaxed to catch borderline cases like 0.21)
    df_temp = df[mask]
    
    # Filter candidates first
    ai_candidates = df_temp[df_temp['lgbm_score'] >= 0.18]
    top_ai = ai_candidates.nlargest(20, 'lgbm_score')
    
    # 2. VIP Keyword Top 10 (Safety net)
    # VIP THRESHOLD: Removed rigorous threshold (>= 0.01) to ensure KEYWORD matches always show up
    vip_pattern = '|'.join(VIP_KEYWORDS)
    has_vip = df_temp[
        df_temp['title'].str.contains(vip_pattern, case=False, na=False) |
        df_temp['summary'].fillna('').str.contains(vip_pattern, case=False, na=False)
    ]
    
    # Safety net: Show if score is non-zero (avoid absolute failures, but trust keywords)
    # Reverting to STRICT Top 5 LIMIT as per user request (Total ~25 articles)
    vip_candidates = has_vip[has_vip['lgbm_score'] >= 0.01]
    top_vip = vip_candidates.nlargest(5, 'lgbm_score')
    
    # 3. Combine and remove duplicates â†’ ~25 articles
    filtered_df = pd.concat([top_ai, top_vip]).drop_duplicates(subset=['url'])
else:
    filtered_df = df[mask]

# Sorting Logic
if sort_mode == "AI Relevance" and 'final_score' in df.columns:
    filtered_df = filtered_df.sort_values('final_score', ascending=False)
elif sort_mode == "Category":
    filtered_df = filtered_df.sort_values('category', ascending=True)
elif sort_mode == "Keyword":
     if 'keywords' in df.columns:
        filtered_df = filtered_df.sort_values('keywords', ascending=True)
else: # Default
    filtered_df = filtered_df.sort_values('published_date', ascending=False)


# Display Metrics
st.markdown(f"**Total Articles:** {len(filtered_df)}")
st.divider()

# Article List Display - Grouped by Category
if filtered_df.empty:
    st.info("No articles found.")
else:
    # Define category display order (priority categories first)
    category_priority = ['Distribution', 'BD', 'Client', 'Zuellig']
    all_categories = filtered_df['category'].unique()
    
    # Sort: priority categories first, then others alphabetically
    sorted_categories = [cat for cat in category_priority if cat in all_categories]
    sorted_categories += sorted([cat for cat in all_categories if cat not in category_priority])
    
    # Group by category
    for category_name in sorted_categories:
        category_df = filtered_df[filtered_df['category'] == category_name]
        
        # Translate category name if needed
        display_category = translate_text(category_name) if use_english else category_name
        
        # Category Header with count (larger font size)
        st.markdown(f"""
        <div style="margin-top: 20px; margin-bottom: 15px;">
            <h3 style="font-size: 22px; color: #006666; border-bottom: 2px solid #0ABAB5; padding-bottom: 8px;">
                ğŸ“‚ {display_category} <span style="color: #888; font-size: 18px;">({len(category_df)} articles)</span>
            </h3>
        </div>
        """, unsafe_allow_html=True)
        
        for _, row in category_df.iterrows():
            title = row['title']
            summary = row.get('summary', '')
            category = row['category']
            date = row['published_date']
            keywords = row.get('keywords', '')
            
            # Translation Logic
            if use_english:
                title, summary, keywords = translate_article_batch(title, summary, keywords)

            st.markdown(f"""
            <div class="article-card">
                <div style="font-size: 16px; line-height: 1.5; color: #333;">
                    <a href="{row['url']}" target="_blank" style="font-size: 18px; font-weight: bold; text-decoration: none; color: #008080;">{title}</a>
                    <span style="color: #666;"> | {date} | {keywords}</span>
                </div>
                <div style="font-size: 16px; margin-top: 8px; color: #555; line-height: 1.6;">
                    {summary}
                </div>
            </div>
            """, unsafe_allow_html=True)

# --- KakaoTalk Summary Generator (New Feature) ---
# Moved to Sidebar as requested for better accessibility
with st.sidebar:
    st.divider()
    st.subheader("ğŸ’¬ Kakao Update")
    if st.button("ğŸ“ Create Summary"):
        with st.spinner("Selecting best articles and formatting..."):
            # 1. Use SAME filtered data as dashboard (respects AI Only filter)
            k_df = filtered_df.copy()
            
            # 2. Competitor Exclusion
            COMPETITORS = ["ì§€ì˜¤ì˜", "DKSH", "ë¸”ë£¨ì— í…", "ë°”ë¡œíŒœ", "ìš©ë§ˆ", "ì‰¥ì»¤", "DHL", "LXíŒí† ìŠ¤", "CJ"]
            def has_competitor(text):
                return any(comp in str(text) for comp in COMPETITORS)
            
            k_df = k_df[~k_df['title'].apply(has_competitor)]
            
            # 3. Use top 20 from dashboard (already category-balanced)
            sort_col = 'final_score' if 'final_score' in k_df.columns else 'published_date'
            k_df = k_df.sort_values(sort_col, ascending=False).head(20)
            
            # 4. Split into Distribution and Pharma Industry
            NEGATIVE_KEYWORDS = ["ê³¼ì§•ê¸ˆ", "í–‰ì •ì²˜ë¶„", "ì ë°œ", "ìœ„ë°˜", "ê²€ì°°", "ì†Œì†¡", "ë¶ˆë§Œ", "ë§¤ê°", "ì² ìˆ˜"]
            
            def is_distribution_article(row):
                category = row.get('category', '')
                text = str(row['title']) + " " + str(row.get('summary', ''))
                
                # Distribution category
                if category == 'Distribution':
                    return True
                
                # Supply Issues
                if category == 'Supply Issues':
                    return True
                
                # Zuellig Positive (no negative keywords)
                if category == 'Zuellig':
                    if not any(neg in text for neg in NEGATIVE_KEYWORDS):
                        return True
                
                return False
            
            dist_df = k_df[k_df.apply(is_distribution_article, axis=1)].head(10)
            ind_df = k_df[~k_df.apply(is_distribution_article, axis=1)].head(10)
            
            # 5. Format Output
            header_dist = "ğŸ“¦ [ì˜ì•½í’ˆ ìœ í†µ (Distribution)]"
            header_ind = "ğŸ¢ [ì œì•½ ì—…ê³„ (Pharma Industry)]"
            msg_none = "- (ê´€ë ¨ ì£¼ìš” ê¸°ì‚¬ ì—†ìŒ)"
            footer_ai = "\n(AI ì„ ì • ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.)"
            footer_rec = "\n(ìµœì‹ ìˆœ ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.)"
            
            if use_english:
                header_dist = "ğŸ“¦ [Distribution News]"
                header_ind = "ğŸ¢ [Pharma Industry News]"
                msg_none = "- (No major articles found)"
                footer_ai = "\n(AI Selected Top News)"
                footer_rec = "\n(Latest Top News)"

            kakao_msg = f"[ZP Market Monitoring Weekly Update]\nğŸ“… Period: {start_date} ~ {end_date}\n\n"
            
            kakao_msg += f"{header_dist}\n"
            if dist_df.empty:
                kakao_msg += f"{msg_none}\n"
            else:
                for _, row in dist_df.iterrows():
                    title = row['title']
                    summary_text = row.get('summary', '')
                    keywords = row.get('keywords', '')
                    date = row.get('published_date', '')
                    
                    if use_english:
                         title, summary_text, _ = translate_article_batch(title, summary_text, keywords)
                    
                    kakao_msg += f"{title} | {date}\n{summary_text}\n{row['url']}\n\n"
                    
            kakao_msg += f"\n{header_ind}\n"
            if ind_df.empty:
                kakao_msg += f"{msg_none}\n"
            else:
                for _, row in ind_df.iterrows():
                    title = row['title']
                    summary_text = row.get('summary', '')
                    keywords = row.get('keywords', '')
                    date = row.get('published_date', '')
                    
                    if use_english:
                         title, summary_text, _ = translate_article_batch(title, summary_text, keywords)
                         
                    kakao_msg += f"{title} | {date}\n{summary_text}\n{row['url']}\n\n"
            
            if 'final_score' in k_df.columns:
                kakao_msg += footer_ai
            else:
                kakao_msg += footer_rec
            
            # Add Disclaimer (Cushion Comment)
            if use_english:
                kakao_msg += "\n\nâ„¹ï¸ Note: This summary is automatically generated by AI. Please refer to the original link for full details."
            else:
                kakao_msg += "\n\nâ„¹ï¸ ì•Œë¦¼: ë³¸ ìš”ì•½ì€ AI ëª¨ë¸ì„ í†µí•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë‚´ìš©ì€ ì›ë¬¸ì„ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."

            # Display - Using st.code with built-in copy button
            st.success("âœ… Summary Generated! Use the copy button (top right) to copy.")
            st.code(kakao_msg, language=None)

