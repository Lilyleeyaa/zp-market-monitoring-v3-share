"""
External Weekly Dashboard - ì™¸ë¶€ìš© (ê²½ìŸì‚¬ ì œì™¸)
"""
import streamlit as st
import sys
import os
import requests
import json
import time

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from auth.simple_auth import authenticate, get_current_user
from scripts.config import get_excluded_keywords, should_exclude_article

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ZP Market Monitoring - MNC_BD",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ì¸ì¦ (ë‚´ë¶€/ì™¸ë¶€ ëª¨ë‘ ê°€ëŠ¥)
email, access_level = authenticate(mode='weekly')

# [Admin/Internal Only] Show external email list for verification
if access_level == 'internal':
    with st.sidebar.expander("ğŸ“§ External Emails (Internal Only)"):
        try:
            # Construct path to external_users.txt relative to this script
            user_list_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'auth', 'external_users.txt')
            if os.path.exists(user_list_path):
                with open(user_list_path, 'r', encoding='utf-8') as f:
                    emails = f.read()
                st.text_area("Registered Emails", emails, height=300)
            else:
                st.warning("external_users.txt not found")
        except Exception as e:
            st.error(f"Error loading emails: {e}")

# ëŒ€ì‹œë³´ë“œ ë©”ì¸ ì½”ë“œ
# --- Data Loading Logic (Synced with Internal) ---
import pandas as pd
import glob
import datetime
from datetime import datetime, timedelta

def get_weekly_date_range():
    today = datetime.now().date()
    start_date = today - timedelta(days=7)
    return start_date, today

# ====================
# Filter Logic (Ported from Internal for Consistency)
# ====================
EXCLUDED_KEYWORDS = [
    "ë„¤ì´ë²„ ë°°ì†¡", "ë„¤ì´ë²„ ì‡¼í•‘", "ë„¤ì´ë²„ í˜ì´", "ë„ì°©ë³´ì¥", 
    "ì¿ íŒ¡", "ë°°ë‹¬ì˜ë¯¼ì¡±", "ìš”ê¸°ìš”", "ë¬´ì‹ ì‚¬", "ì»¬ë¦¬", "ì•Œë¦¬ìµìŠ¤í”„ë ˆìŠ¤", "í…Œë¬´",
    "ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì „ì„¸", "ë§¤ë§¤", "ì²­ì•½", "ê±´ì„¤", 
    "ê¸ˆë¦¬ ì¸í•˜", "ì£¼ì‹ ê°œì¥", "í™˜ìœ¨", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¦ì‹œ", "ìƒí•œê°€", 
    "ì£¼ê°€", "ì£¼ì‹", "ëª©í‘œì£¼ê°€", "íŠ¹ì§•ì£¼", "ê¸‰ë“±",
    "ì—¬í–‰", "í˜¸í…”", "í•­ê³µê¶Œ", "ì˜ˆëŠ¥", "ë“œë¼ë§ˆ", "ì¶•êµ¬", "ì•¼êµ¬", "ì˜¬ë¦¼í”½", "ì—°ì˜ˆ", "ê³µì—°", "ë®¤ì§€ì»¬", "ì „ì‹œíšŒ", "ê´€ëŒ",
    "ì´ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ì „ê¸°ì°¨", "ë°˜ë„ì²´", "ë””ìŠ¤í”Œë ˆì´", "ì¡°ì„ ", "ì² ê°•",
    "ì±„ìš©", "ì‹ ì…ì‚¬ì›", "ê³µì±„", "ì›ì„œì ‘ìˆ˜", "ê³ ì–‘ì´",
    "ìŒì‹", "1ì¸ë¶„", "ë¬¸ì—¬ëŠ”", "ëŒ€ì „ì‹œì¥", "ì´ë®¨ì˜¨ì‹œì•„", "ì—ìŠ¤ë°”ì´ì˜¤ë©”ë”•ìŠ¤"
]

GENERIC_KEYWORDS = ["ê³„ì•½", "M&A", "ì¸ìˆ˜", "í•©ë³‘", "íˆ¬ì", "ì œíœ´", "CJ"]
PHARMA_CONTEXT_KEYWORDS = ["ì œì•½", "ë°”ì´ì˜¤", "ì‹ ì•½", "ì„ìƒ", "í—¬ìŠ¤ì¼€ì–´", "ì˜ë£Œ", "ë³‘ì›", "ì•½êµ­", "ì¹˜ë£Œì œ", "ë°±ì‹ ", "ì§„ë‹¨", "ë¬¼ë¥˜", "ìœ í†µ", "ê³µê¸‰"]

def is_noise_article(row):
    # Check Title + Summary + Content (Body)
    text = str(row['title']) + " " + str(row.get('summary', '')) + " " + str(row.get('content', ''))
    
    # 1. Check Explicit Exclusions
    for exc in EXCLUDED_KEYWORDS:
        if exc in text:
            return True
            
    # 2. Homonym Check: "ì œì•½" (Constraint vs Pharma)
    if "ì œì•½" in text:
        if any(x in text for x in ["ì‹œê°„ ì œì•½", "ê³µê°„ ì œì•½", "ë¬¼ë¦¬ì  ì œì•½", "ë°œì „ ì œì•½", "í™œë™ ì œì•½"]):
            if not any(pk in text for pk in PHARMA_CONTEXT_KEYWORDS if pk != "ì œì•½"):
                return True
                
    # 3. Context Check for Generics (M&A, Investment)
    if any(k in text for k in GENERIC_KEYWORDS):
        if not any(pk in text for pk in PHARMA_CONTEXT_KEYWORDS):
            return True
            
    return False

# Configure Gemini API
GENAI_API_KEY = os.getenv("GENAI_API_KEY")
if not GENAI_API_KEY:
    try:
        GENAI_API_KEY = st.secrets["GENAI_API_KEY"]
    except:
        GENAI_API_KEY = ""

if not GENAI_API_KEY:
    pass  # Translation will fail gracefully

GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GENAI_API_KEY}"

def translate_text(text, target='en'):
    if not text: return ""
    
    # 1. Try Gemini API first (High Quality) with Retry Logic
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Construct explicit prompt with glossary context (If variables exist, else empty)
            # External dashboard might not have full glossary defined globally yet?
            # I will check if KEYWORD_MAPPING is defined.
            glossary_context = "" 
            # (Assuming KEYWORD_MAPPING might be defined below, I should check file content first)
            
            prompt = f"""
            You are a professional pharmaceutical translator. 
            Translate the following Korean text to English.
            
            Rules:
            1. Maintain professional industry terminology.
            2. Use the specific glossary below for strict term matching:
            {glossary_context}
            
            Text to translate:
            "{text}"
            
            Output only the translated English text, no explanations.
            """
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }]
            }
            
            headers = {'Content-Type': 'application/json'}
            response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload), timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    return result['candidates'][0]['content']['parts'][0]['text'].strip()
            elif response.status_code == 429:
                if attempt < max_retries - 1:
                    time.sleep(2) # Wait 2s before retry
                    continue
            else:
                print(f"[Gemini API Error] {response.status_code}: {response.text}")
                break 
                
        except Exception as e:
            print(f"[Gemini Exception] {e}")
            break
            
    # 2. Fallback
    try:
        from deep_translator import GoogleTranslator
        return GoogleTranslator(source='auto', target=target).translate(text)
    except:
        return text

# ====================
# Data Loading (Synced with Internal)
# ====================
@st.cache_data(ttl=60, show_spinner=False)
def load_weekly_data():
    try:
        base_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "articles_raw")
        if not os.path.exists(base_dir):
            base_dir = "../data/articles_raw"
            
        ranked_files = sorted(glob.glob(os.path.join(base_dir, "articles_ranked_*.csv")))
        if not ranked_files:
            return pd.DataFrame(), "No Data"
            
        latest_file = ranked_files[-1]
        df = pd.read_csv(latest_file, encoding='utf-8-sig')
        
        if 'published_date' in df.columns:
            df['published_date'] = pd.to_datetime(df['published_date']).dt.date

        if 'category' not in df.columns:
            df['category'] = 'General'

        if 'keywords' not in df.columns:
            df['keywords'] = ''
            
        # Noise Filter
        if not df.empty:
            df['is_noise'] = df.apply(is_noise_article, axis=1)
            df = df[~df['is_noise']]
            
        return df, os.path.basename(latest_file)
    except Exception as e:
        return pd.DataFrame(), str(e)

df, filename = load_weekly_data()

if df.empty:
    st.error(f"ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
    st.stop()


# --- Constants ---
KEYWORD_MAPPING = {
    'Bí˜•ê°„ì—¼': 'Hepatitis B',
    'Cí˜•ê°„ì—¼': 'Hepatitis C',
    'CDMO': 'CDMO',
    'CMD': 'CMD',
    'CSO': 'CSO',
    'CAR-T': 'CAR-T',
    'GLP-1': 'GLP-1',
    'ADC': 'ADC',
    'HIV': 'HIV',
    'M&A': 'M&A',
    'mRNA': 'mRNA',
    'R&D': 'R&D',
    'AI': 'AI',
    'ê°€ë‹¤ì‹¤': 'Gardasil',
    'ê³ í˜ˆì••': 'Hypertension',
    'ê³¨ë‹¤ê³µì¦': 'Osteoporosis',
    'êµ­ê°€í•„ìˆ˜ì˜ˆë°©ì ‘ì¢…': 'NIP',
    'ê¸ˆì—°ì¹˜ë£Œ': 'Smoking Cessation',
    'ë‹¹ë‡¨ë³‘': 'Diabetes',
    'ëŒ€ìƒí¬ì§„': 'Shingles',
    'ë…ê°': 'Flu',
    'ë§ˆì•½ë¥˜': 'Narcotics',
    'ë§Œì„±ì§ˆí™˜': 'Chronic Disease',
    'ë©´ì—­í•­ì•”ì œ': 'Immuno-oncology',
    'ë°”ì´ì˜¤ì‹œë°€ëŸ¬': 'Biosimilar',
    'ë°±ì‹ ': 'Vaccine',
    'ë¹„ë§Œ': 'Obesity',
    'ì‚°ì •íŠ¹ë¡€': 'Special Calc',
    'ìƒê¸‰ì¢…í•©ë³‘ì›': 'Tertiary Hosp',
    'ì‹ ì•½': 'New Drug',
    'ì‹¬í˜ˆê´€': 'Cardiovascular',
    'ì•”': 'Cancer',
    'ì•½ê°€': 'Drug Price',
    'ì•½êµ­': 'Pharmacy',
    'ì—°ë§ì •ì‚°': 'Tax Adj',
    'ì´ìƒì§€ì§ˆí˜ˆì¦': 'Dyslipidemia',
    'ì„ìƒ': 'Clinical Trial',
    'ìê°€ë©´ì—­ì¹ í™˜': 'Autoimmune',
    'ì œë„¤ë¦­': 'Generic',
    'ì¢…ì–‘': 'Tumor',
    'ì¤‘ì¦ì§ˆí™˜': 'Severe Disease',
    'ì¹˜ë§¤': 'Dementia',
    'íƒˆëª¨': 'Hair Loss',
    'íŠ¹í—ˆ': 'Patent',
    'íì•”': 'Lung Cancer',
    'í’ˆì ˆ': 'Out of Stock',
    'í•­ì•”ì œ': 'Anticancer',
    'í—¬ìŠ¤ì¼€ì–´': 'Healthcare',
    'í˜‘íšŒ': 'Association',
    'í¬ê·€ì§ˆí™˜': 'Rare Disease',
    'ì§€í”¼í…Œë¼í“¨í‹±ìŠ¤': 'ZP Therapeutics',
    'ì§€í”¼': 'ZP Therapeutics',
    'ì§€í”¼ í…Œë¼í“¨í‹±ìŠ¤': 'ZP Therapeutics'
}

# ====================
# Translation Components (Copied from Internal)
# ====================
EXTRA_GLOSSARY = {
    "ë°ì¼ë¦¬íŒœ": "Daily Pharm",
    "ì•½ì‚¬ê³µë¡ ": "Yaksagongron",
    "ë©”ë””íŒŒë‚˜": "Medipana",
    "ì˜í•™ì‹ ë¬¸": "Medical Times",
    "ì²­ë…„ì˜ì‚¬": "Doctor's News",
    "ë‰´ìŠ¤1": "News1",
    "ë‰´ì‹œìŠ¤": "Newsis",
    "ì²˜ë°©ê¶Œ ì§„ì…": "Entry into Prescription Market",
    "ì²˜ë°©ê¶Œ": "Prescription Market",
    "ê¸‰ì—¬ í™•ëŒ€": "Reimbursement Expansion",
    "ê¸‰ì—¬": "Reimbursement",
    "ë¹„ê¸‰ì—¬": "Non-Reimbursement",
    "ì•½ê°€ ì¸í•˜": "Price Cut",
    "ì•½ê°€": "Drug Price",
    "ì œë„¤ë¦­": "Generic",
    "ì˜¤ë¦¬ì§€ë„": "Original",
    "í’ˆì ˆ": "Out of Stock",
    "ê³µê¸‰ë¶€ì¡±": "Supply Shortage",
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption",
    "ì„ìƒ": "Clinical Trial",
    "í—ˆê°€": "Approval",
    "ì‹ì•½ì²˜": "MFDS",
    "ì‹¬í‰ì›": "HIRA",
    "ê±´ë³´ê³µë‹¨": "NHIS",
    "ì•±ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ì—¡ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ìƒê¸‰ì¢…í•©ë³‘ì›": "Tertiary General Hospital",
    "ê±´ê¸°ì‹": "Health Functional Food",
    "ì¥´ë¦­": "Zuellig", 
    "ì¥´ë¦­íŒŒë§ˆ": "Zuellig Pharma",
    "ì¥´ë¦­ì½”ë¦¬ì•„": "Zuellig Pharma Korea",
    "ì¥´ë¦­ íŒŒë§ˆ": "Zuellig Pharma",
    "ë‹ˆì½”í‹´ì—˜": "Nicotinell",
}

import re # For robust replacement

# API Key Security: Load from Streamlit Secrets or Environment Variable
try:
    GENAI_API_KEY = st.secrets["GENAI_API_KEY"]
except:
    import os
    GENAI_API_KEY = os.getenv("GENAI_API_KEY", "")

if not GENAI_API_KEY:
    # Placeholder for local development if secrets not set (Translation will fail gracefully)
    pass
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent?key={GENAI_API_KEY}"

@st.cache_data(show_spinner=False, ttl=3600)
def translate_text(text, target='en'):
    # Cache Version: v8 (error capture)
    if not text: return ""
    
    full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}

    # â”€â”€ Gemini ë²ˆì—­ (quota ì†Œì§„ ì‹œ ì£¼ì„ í•´ì œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # [ì£¼ì„ì²˜ë¦¬ ìœ ì§€ - quota ì†Œì§„]
    # â”€â”€ Gemini ë â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # deep_translator (Google Translate) + glossary ì¹˜í™˜
    try:
        from deep_translator import GoogleTranslator
        processed_text = text
        sorted_terms = sorted(full_glossary.keys(), key=len, reverse=True)
        for kr_term in sorted_terms:
            if kr_term in processed_text:
                processed_text = processed_text.replace(kr_term, full_glossary[kr_term])
        translated = GoogleTranslator(source='ko', target=target).translate(processed_text)
        translated = re.sub(r'nicotine\s*l', 'Nicotinell', translated, flags=re.IGNORECASE)
        return translated
    except Exception as e:
        # ì—ëŸ¬ ë‚´ìš©ì„ session_stateì— ì €ì¥ (ë””ë²„ê·¸ìš©)
        if 'translation_error' not in st.session_state:
            st.session_state['translation_error'] = str(e)
        return text  # ì›ë³¸ ë°˜í™˜

@st.cache_data(show_spinner=False, ttl=3600)
def translate_article_batch(title, summary, keywords):  # Cache v8
    if not title and not summary: return title, summary, keywords
    combined_text = f"Title: {title}\nSummary: {summary}\nKeywords: {keywords}"
    result_text = translate_text(combined_text)
    
    t_title, t_summary, t_keywords = title, summary, keywords
    try:
        lines = result_text.split('\n')
        for line in lines:
            if line.startswith("Title:") or line.startswith("Title :"):
                t_title = line.split(":", 1)[1].strip()
            elif line.startswith("Summary:") or line.startswith("Summary :"):
                t_summary = line.split(":", 1)[1].strip()
            elif line.startswith("Keywords:") or line.startswith("Keywords :"):
                t_keywords = line.split(":", 1)[1].strip()
    except:
        pass
    return t_title, t_summary, t_keywords


# --- Top Control Bar (Filters) ---
st.markdown("### ğŸ” Filters & Settings")

f_col1, f_col2, f_col3, f_col4, f_col5, f_col6 = st.columns([1.5, 2, 2, 2, 2, 1.5])

with f_col1:
    lang_opt = st.selectbox("ğŸŒ Language", ["Korean", "English"], index=0)
    use_english = (lang_opt == "English")

with f_col2:
    start_week, end_week = get_weekly_date_range()
    if 'published_date' in df.columns:
        min_date = df['published_date'].min()
        max_date = df['published_date'].max()
        default_start = max(min_date, start_week) if min_date else start_week
        default_end = min(max_date, end_week) if max_date else end_week
        date_range = st.date_input("ğŸ“… Date Range", [default_start, default_end])
        if isinstance(date_range, list) and len(date_range) == 2:
            start_date, end_date = date_range
        else:
            start_date, end_date = default_start, default_end
    else:
        start_date, end_date = None, None

with f_col3:
    all_categories = sorted(df['category'].dropna().unique().tolist())
    selected_categories = st.multiselect("ğŸ“‚ Category", all_categories, default=[])

# [DEBUG] Translation error display
if st.session_state.get('translation_error'):
    st.warning(f"âš ï¸ Translation error: {st.session_state['translation_error']}")

# Dynamic Keyword Filter Preparation
temp_mask = pd.Series([True] * len(df))

# Explicit Exclusion for External Dashboard (User Request)
EXCLUDED_KEYWORDS_EXT = ["ê³ ì–‘ì´"]
if EXCLUDED_KEYWORDS_EXT:
    pat_ext = '|'.join(EXCLUDED_KEYWORDS_EXT)
    temp_mask = temp_mask & ~(
        df['title'].str.contains(pat_ext, case=False, na=False) |
        df['summary'].fillna('').str.contains(pat_ext, case=False, na=False) |
        df['keywords'].fillna('').str.contains(pat_ext, case=False, na=False)
    )

if start_date and end_date:
    temp_mask = temp_mask & (df['published_date'] >= start_date) & (df['published_date'] <= end_date)

if selected_categories:
    temp_mask = temp_mask & (df['category'].isin(selected_categories))

# Apply excluded keywords FIRST to the kw extraction source
# Competitor keywords excluded from external dashboard
COMPETITOR_KEYWORDS = ["ëŒ€ì›…", "ì¢…ê·¼ë‹¹", "í•œë¯¸ì•½í’ˆ", "ìœ í•œì–‘í–‰", "ë…¹ì‹­ì", "ì¼ë™ì œì•½", "ë³´ë ¹", "ë™ì•„ST", "JWì¤‘ì™¸", "ê´‘ë™ì œì•½"]
if COMPETITOR_KEYWORDS:
    pat = '|'.join(COMPETITOR_KEYWORDS)
    safe_kw_mask = ~(
        df['title'].str.contains(pat, case=False, na=False) |
        df['summary'].fillna('').str.contains(pat, case=False, na=False) |
        df['keywords'].fillna('').str.contains(pat, case=False, na=False)
    )
    df_kw_source = df[temp_mask & safe_kw_mask]
else:
    df_kw_source = df[temp_mask]

with f_col4:
    available_keywords = []
    if 'keywords' in df_kw_source.columns:
        # Extract individual keywords if they are comma-separated strings
        all_kws = []
        for k_str in df_kw_source['keywords'].dropna().astype(str):
            for k in k_str.split(','):
                k = k.strip()
                if k: all_kws.append(k)
        available_keywords = sorted(list(set(all_kws)))
    
    if use_english:
        keyword_options = [KEYWORD_MAPPING.get(k, k) for k in available_keywords]
        en_to_kr = {KEYWORD_MAPPING.get(k, k): k for k in available_keywords}
    else:
        keyword_options = available_keywords
    
    selected_keywords_display = st.multiselect("ğŸ”‘ Keyword", keyword_options, default=[])
    
    if use_english:
        selected_keywords = [en_to_kr.get(k, k) for k in selected_keywords_display]
    else:
        selected_keywords = selected_keywords_display

with f_col5:
    sort_opts = ["AI Relevance", "Latest Date", "Category", "Keyword"]
    sort_mode = st.selectbox("ğŸ“Š Sort By", sort_opts)

with f_col6:
    # Changed to Checkbox for "AI Only" matching Internal
    show_ai_only = st.checkbox("ğŸ¤– AI Only", value=True, help="Show only AI recommended articles")

# --- Logic Phase 1: Global Exclusion (External Security) ---
excluded_keywords = COMPETITOR_KEYWORDS.copy()

# User Request: Force Exclude 'Cat' in External Dashboard
if "ê³ ì–‘ì´" not in excluded_keywords:
    excluded_keywords.append("ê³ ì–‘ì´")

df_safe = df.copy()

if excluded_keywords:
    pattern = '|'.join(excluded_keywords)
    mask_sensitive = (
        df_safe['title'].str.contains(pattern, case=False, na=False) |
        df_safe['summary'].fillna('').str.contains(pattern, case=False, na=False) | 
        df_safe['keywords'].fillna('').str.contains(pattern, case=False, na=False)
    )
    df_safe = df_safe[~mask_sensitive]

# --- Logic Phase 2: User Filters ---
mask = pd.Series([True] * len(df_safe))
if start_date and end_date:
    mask = (df_safe['published_date'] >= start_date) & (df_safe['published_date'] <= end_date)

if selected_categories:
    mask = mask & (df_safe['category'].isin(selected_categories))

if selected_keywords:
    # Check if ANY of the selected keywords are present in the article's keyword string
    # Simple 'isin' doesn't work for comma-separated, so we use string verify
    # But for performance and standard logic:
    # Construct regex or use apply. Internal uses isin directly if normalized? 
    # Internal code was: mask = mask & (df['keywords'].isin(selected_keywords))
    # This implies Internal assumes single keyword or exact match? 
    # Actually internal loader likely didn't explode keywords?
    # Let's use str.contains logic for safety with partial matches
    kw_pattern = '|'.join([k for k in selected_keywords])
    mask = mask & (df_safe['keywords'].fillna('').str.contains(kw_pattern, na=False))

df_filtered = df_safe[mask]

# --- Logic Phase 3: Quota & Rank ---
if 'final_score' not in df_filtered.columns and 'lgbm_score' in df_filtered.columns:
    df_filtered['final_score'] = df_filtered['lgbm_score']

# Sort by Score for selection
df_sorted = df_filtered.sort_values(by='final_score', ascending=False)

# Quota Logic (Top 4 per category) OR Show All
if show_ai_only:
    # --- Strict AI Mode: Limit to Top 20 ---
    balanced_selection = []
    selected_urls = set()
    categories = df_sorted['category'].unique()

    for cat in ['Distribution', 'Client', 'BD', 'Zuellig']:
        if cat in categories:
            cat_articles = df_sorted[df_sorted['category'] == cat].head(4)
            balanced_selection.append(cat_articles)
            selected_urls.update(cat_articles['url'].tolist())

    if balanced_selection:
        df_balanced = pd.concat(balanced_selection)
    else:
        df_balanced = pd.DataFrame()

    # Backfill remaining slots
    remaining_slots = 20 - len(df_balanced)
    if remaining_slots > 0:
        remaining_candidates = df_sorted[~df_sorted['url'].isin(selected_urls)]
        df_fill = remaining_candidates.head(remaining_slots)
        df_visible = pd.concat([df_balanced, df_fill])
    else:
        df_visible = df_balanced.head(20)
else:
    # --- Show All Mode ---
    # Just show all filtered articles (Competitors already excluded in Phase 1)
    df_visible = df_sorted

# Final Sorting for Display
if sort_mode == "AI Relevance":
    if 'final_score' in df_visible.columns:
        df_visible = df_visible.sort_values('final_score', ascending=False)
elif sort_mode == "Category":
    df_visible = df_visible.sort_values('category', ascending=True)
else:
    df_visible = df_visible.sort_values('published_date', ascending=False)

# Metrics
st.markdown(f"**Total Articles:** {len(df_visible)}")
st.divider()

# --- Display Logic (Tiffany Blue Theme - Exact Match with Internal) ---
st.markdown("""
<style>
    /* Global Background & Font */
    .stApp {
        background-color: #F0F8F8; /* Very Light Teal/Grey */
    }
    
    /* Header/Title */
    h1 {
        color: #006666 !important; /* Deep Teal */
    }
    
    /* Article Card Styles */
    .article-card {
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 15px;
        background-color: #ffffff; /* White card */
        border-left: 5px solid #0ABAB5; /* Tiffany Blue Accent */
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    .article-title {
        font-size: 18px;
        font-weight: bold;
        color: #008080; /* Teal */
        text-decoration: none;
    }
    .article-title:hover {
        color: #0ABAB5; /* Tiffany Blue on Hover */
        text_decoration: underline;
    }
    
    .article-meta {
        font-size: 12px;
        color: #888;
    }
    
    .category-badge {
        background-color: #E0F2F1; /* Light Teal background */
        color: #00695C; /* Dark Teal text */
        padding: 4px 8px;
        border-radius: 12px;
        font-size: 12px;
        font-weight: 500;
        margin-left: 5px;
    }

    .article-summary {
        font-size: 14px;
        color: #444;
        margin-top: 8px;
        line-height: 1.6;
    }
    
    .keyword-tag {
        background-color: #f1f3f4;
        color: #5f6368;
        padding: 2px 6px;
        border-radius: 4px;
        font-size: 11px;
        margin-right: 5px;
    }
    
    /* Button Styles */
    .stButton>button {
        background-color: #0ABAB5 !important;
        color: white !important;
        border: none;
    }
</style>
""", unsafe_allow_html=True)

if df_visible.empty:
    st.warning("í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    # Display Articles by Category matching Internal Style
    # Priority: Zuellig -> Distribution -> BD -> Client -> Others
    category_priority = ['Zuellig', 'Distribution', 'BD', 'Client']
    all_categories = df_visible['category'].unique()
    
    # Sort categories forcing Zuellig first
    sorted_categories = [cat for cat in category_priority if cat in all_categories]
    sorted_categories += sorted([cat for cat in all_categories if cat not in category_priority])
    
    for category_name in sorted_categories:
        category_df = df_visible[df_visible['category'] == category_name]
        
        if category_df.empty:
            continue
            
        # Clean Header (No border, matching Internal style)
        st.markdown(f"""
        <div style="margin-top: 20px; padding-bottom: 5px;">
            <span style="font-size: 24px; font-weight: bold; color: #006666;">{category_name}</span>
            <span style="font-size: 16px; color: #666; margin-left: 10px;">({len(category_df)} articles)</span>
        </div>
        """, unsafe_allow_html=True)
        
        for _, row in category_df.iterrows():
            title = row['title']
            summary = row.get('summary', '')
            date = row.get('published_date', '')
            keywords = row.get('keywords', '')
            url = row.get('url', '#')
            
            # --- FAILSAFE EXCLUSION (User Request) ---
            # Check Original Korean
            if "ê³ ì–‘ì´" in title or "ê³ ì–‘ì´" in summary or "ê³ ì–‘ì´" in keywords:
                continue

            # Translate if needed
            if use_english:
                title, summary, keywords_trans = translate_article_batch(title, summary, keywords)
                keywords = keywords_trans
                
                # Check Translated Text
                if "Cat" in title or "Cat" in summary:
                    continue
            
            # Internal Style: Title ... | Date | Keywords
            st.markdown(f'''
            <div class="article-card">
                <div style="font-size: 16px; line-height: 1.5; color: #333;">
                    <a href="{url}" target="_blank" style="font-size: 18px; font-weight: bold; text-decoration: none; color: #008080;">{title}</a>
                    <span style="color: #666; font-size: 12px;"> | {date} | {keywords}</span>
                </div>
                <div style="font-size: 16px; margin-top: 8px; color: #555; line-height: 1.6;">
                    {summary}
                </div>
            </div>
            ''', unsafe_allow_html=True)
