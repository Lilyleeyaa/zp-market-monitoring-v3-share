"""
Internal Weekly Dashboard - ë‚´ë¶€ìš© (ê²½ìŸì‚¬ í¬í•¨)
V2 Design & Filter Logic Restoration
"""
import streamlit as st
import pandas as pd
import sys
import os
import requests
import json
import time
from datetime import datetime, timedelta
import pytz

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from auth.simple_auth import authenticate

# Page configuration
st.set_page_config(
    page_title="ZP Market Monitoring - Internal Weekly",
    page_icon="ğŸ¥",
    layout="wide"
)

# Apply Noto Sans KR font globally (V2 Style)
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');

* {
    font-family: 'Noto Sans KR', sans-serif !important;
}

html, body, div, span, p, h1, h2, h3, h4, h5, h6 {
    font-family: 'Noto Sans KR', sans-serif !important;
}

.stMarkdown, .stText, .stButton button, .stSelectbox, .stMultiSelect {
    font-family: 'Noto Sans KR', sans-serif !important;
}

text-area, input, .stTextArea textarea, .stTextInput input {
    font-family: 'Noto Sans KR', sans-serif !important;
}

[data-testid="stMarkdownContainer"] {
    font-family: 'Noto Sans KR', sans-serif !important;
}

/* Global Background & Font */
.stApp {
    background-color: #F0F8F8; /* Very Light Teal/Grey */
}

/* Header/Title */
h1 {
    color: #006666 !important; /* Deep Teal */
}

/* Article Card Styles */
.article-card {
    padding: 15px;
    border-radius: 8px;
    margin-bottom: 15px;
    background-color: #ffffff; /* White card */
    border-left: 5px solid #0ABAB5; /* Tiffany Blue Accent */
    box-shadow: 0 2px 5px rgba(0,0,0,0.05);
}

.article-title {
    font-size: 18px;
    font-weight: bold;
    color: #008080; /* Teal */
    text-decoration: none;
}
.article-title:hover {
    color: #0ABAB5; /* Tiffany Blue on Hover */
    text-decoration: underline;
}

.article-meta {
    font-size: 12px;
    color: #888;
}

.category-badge {
    background-color: #E0F2F1; /* Light Teal background */
    color: #00695C; /* Dark Teal text */
    padding: 4px 8px;
    border-radius: 12px;
    font-size: 12px;
    font-weight: 500;
    margin-left: 5px;
}

.article-summary {
    font-size: 14px;
    color: #444;
    margin-top: 8px;
    line-height: 1.6;
}

/* Button Styles */
.stButton>button {
    background-color: #0ABAB5 !important;
    color: white !important;
    border: none;
}
</style>
""", unsafe_allow_html=True)

# ì¸ì¦ (ë‚´ë¶€ ì „ìš©)
email, access_level = authenticate(mode='weekly')

if access_level != 'internal':
    st.error("âŒ ì´ ëŒ€ì‹œë³´ë“œëŠ” ë‚´ë¶€ ì‚¬ìš©ìë§Œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    st.stop()

# ====================
# Translation Components (V2)
# ====================
# Custom Glossary for Pre-translation check
EXTRA_GLOSSARY = {
    "ë°ì¼ë¦¬íŒœ": "Daily Pharm",
    "ì•½ì‚¬ê³µë¡ ": "Yaksagongron",
    "ë©”ë””íŒŒë‚˜": "Medipana",
    "ì˜í•™ì‹ ë¬¸": "Medical Times",
    "ì²­ë…„ì˜ì‚¬": "Doctor's News",
    "ë‰´ìŠ¤1": "News1",
    "ë‰´ì‹œìŠ¤": "Newsis",
    "ì²˜ë°©ê¶Œ ì§„ì…": "Entry into Prescription Market",
    "ì²˜ë°©ê¶Œ": "Prescription Market",
    "ê¸‰ì—¬ í™•ëŒ€": "Reimbursement Expansion",
    "ê¸‰ì—¬": "Reimbursement",
    "ë¹„ê¸‰ì—¬": "Non-Reimbursement",
    "ì•½ê°€ ì¸í•˜": "Price Cut",
    "ì•½ê°€": "Drug Price",
    "ì œë„¤ë¦­": "Generic",
    "ì˜¤ë¦¬ì§€ë„": "Original",
    "í’ˆì ˆ": "Out of Stock",
    "ê³µê¸‰ë¶€ì¡±": "Supply Shortage",
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption",
    "ì„ìƒ": "Clinical Trial",
    "í—ˆê°€": "Approval",
    "ì‹ì•½ì²˜": "MFDS",
    "ì‹¬í‰ì›": "HIRA",
    "ê±´ë³´ê³µë‹¨": "NHIS",
    "ì•±ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ì—¡ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ìƒê¸‰ì¢…í•©ë³‘ì›": "Tertiary General Hospital",
    "ê±´ê¸°ì‹": "Health Functional Food",
    "í”„ë¦¬í•„ë“œ": "Pre-filled",
}

KEYWORD_MAPPING = {
    "ì˜ì•½í’ˆìœ í†µ": "Pharmaceutical Distribution", "ì§€ì˜¤ì˜": "GeoYoung", "DKSH": "DKSH", "ë¸”ë£¨ì— í…": "BlueMtech", "ë°”ë¡œíŒœ": "Baropharm", "ìš©ë§ˆ": "Yongma", "ì‰¥ì»¤": "Schenker", "DHL": "DHL", "LXíŒí† ìŠ¤": "LX Pantos", "CJ": "CJ",
    "ê³µë™íŒë§¤": "Co-Promotion", "ì½”í”„ë¡œëª¨ì…˜": "Co-Promotion", "ìœ í†µê³„ì•½": "Distribution Agreement", "íŒê¶Œ": "Sales Rights", "ë¼ì´ì„ ìŠ¤": "License", "M&A": "M&A", "ì¸ìˆ˜": "Acquisition", "í•©ë³‘": "Merger", "ì œíœ´": "Partnership", "íŒŒíŠ¸ë„ˆì‹­": "Partnership", "ê³„ì•½": "Contract", "ìƒë¬¼í•™ì ì œì œ": "Biologics", "ì½œë“œì²´ì¸": "Cold Chain", "CSO": "CSO", "íŒì´‰ì˜ì—…ì": "Sales Agent", "íŠ¹í—ˆë§Œë£Œ": "Patent Expiry", "êµ­ê°€ë°±ì‹ ": "National Vaccine", "ë°±ì‹ ": "Vaccine",
    "í—ˆê°€": "Approval", "ì‹ ì œí’ˆ": "New Product", "ì¶œì‹œ": "Launch", "ì‹ ì•½": "New Drug", "ì ì‘ì¦": "Indication", "ì œí˜•": "Formulation", "ìš©ëŸ‰": "Dosage",
    "ë³´í—˜ë“±ì¬": "Reimbursement", "ê¸‰ì—¬": "NHI Coverage", "ì•½ê°€": "Drug Price",
    "ì¥´ë¦­": "Zuellig", "ì§€í”¼í…Œë¼í“¨í‹±ìŠ¤": "ZP Therapeutics", "ë¼ë¯¸ì‹¤": "Lamisil", "ì•¡í‹°ë„˜": "Actinum", "ë² íƒ€ë”˜": "Betadine", "ì‚¬ì´í´ë¡œì œìŠ¤íŠ¸": "Cyclogest", "ë¦¬ë¸Œíƒ€ìš”": "Libtayo",
    "í•œë…": "Handok", "MSD": "MSD", "ì˜¤ê°€ë…¼": "Organon", "í™”ì´ì": "Pfizer", "ì‚¬ë…¸í”¼": "Sanofi", "ì•”ì  ": "Amgen", "GSK": "GSK", "ë¡œìŠˆ": "Roche", "ë¦´ë¦¬": "Lilly", "ë…¸ë°”í‹°ìŠ¤": "Novartis", "ë…¸ë³´ë…¸ë””ìŠ¤í¬": "Novo Nordisk", "ë¨¸í¬": "Merck", "ë ˆì½”ë¥´ë‹¤í‹°": "Recordati", "ì…€ì§„": "Celgene", "í…Œë°”í•œë…": "Teva-Handok", "ë² ë§ê±°ì¸ê²”í•˜ì„": "Boehringer Ingelheim", "BMS": "BMS", "ì•„ìŠ¤íŠ¸ë¼ì œë„¤ì¹´": "AstraZeneca", "ì• ë¸Œë¹„": "AbbVie", "íŒŒë§ˆë…¸ë¹„ì•„": "Pharmanovia", "ë¦¬ì œë„¤ë¡ ": "Regeneron", "ë°”ì´ì—˜": "Bayer", "ì•„ìŠ¤í…”ë¼ìŠ¤": "Astellas", "ì–€ì„¼": "Janssen", "ë°”ì´ì˜¤ì  ": "Biogen", "ì…ì„¼": "Ipsen", "ì• ë³´íŠ¸": "Abbott", "ì•ˆí…ì§„": "Antengene", "ë² ì´ì§„": "BeiGene", "ì…€íŠ¸ë¦¬ì˜¨": "Celltrion", "í—¤ì¼ë¦¬ì˜¨": "Haelion", "ì˜¤í ë¼": "Opella", "ì¼„ë·°": "Kenvue", "ë¡œë ˆì•Œ": "L'Oreal", "ë©”ë‚˜ë¦¬ë‹ˆ": "Menarini", "ìœ„ê³ ë¹„": "Wegovy", "ë§ˆìš´ìë¡œ": "Mounjaro",
    "ë‚œì„": "Infertility", "ë¶ˆì„": "Infertility", "í•­ì•”ì œ": "Anticancer",
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption", "ê³µê¸‰ë¶€ì¡±": "Supply Shortage", "í’ˆì ˆ": "Out of Stock", "í’ˆê·€": "Shortage",
}

# Configure Gemini API
GENAI_API_KEY = "AIzaSyD5HUixHFDEeifmY5NhJCnL4cLlxOp7fp0"
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GENAI_API_KEY}"

@st.cache_data(show_spinner=False)
def translate_text(text, target='en'):
    if not text: return ""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
            glossary_context = "\n".join([f"- {k}: {v}" for k, v in full_glossary.items()])
            
            prompt = f"""
            You are a professional pharmaceutical translator. 
            Translate the following Korean text to English.
            
            Rules:
            1. Maintain professional industry terminology.
            2. Use the specific glossary below for strict term matching:
            {glossary_context}
            
            Text to translate:
            "{text}"
            
            Output only the translated English text, no explanations.
            """
            
            payload = {"contents": [{"parts": [{"text": prompt}]}]}
            headers = {'Content-Type': 'application/json'}
            response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload), timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    return result['candidates'][0]['content']['parts'][0]['text'].strip()
            elif response.status_code == 429:
                time.sleep(2)
                continue
            else:
                break
        except Exception as e:
            break
            
    # Fallback: Google Translator
    try:
        from deep_translator import GoogleTranslator
        processed_text = text
        full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
        sorted_terms = sorted(full_glossary.keys(), key=len, reverse=True)
        for kr_term in sorted_terms:
            if kr_term in processed_text:
                processed_text = processed_text.replace(kr_term, full_glossary[kr_term])
        return GoogleTranslator(source='ko', target=target).translate(processed_text)
    except:
        return text

@st.cache_data(show_spinner=False)
def translate_article_batch(title, summary, keywords):
    """
    Title, Summary, Keywordsë¥¼ í•œ ë²ˆì— ë²ˆì—­í•˜ì§€ ì•Šê³  (ë³µì¡ë„ ë•Œë¬¸), ê°œë³„ ë²ˆì—­ í˜¸ì¶œë¡œ ì•ˆì •ì„± í™•ë³´
    (V2ì˜ ë°°ì¹˜ëŠ” íŒŒì‹± ë¡œì§ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ê°œë³„ í˜¸ì¶œë¡œ ë³€ê²½í•˜ê±°ë‚˜ V2 ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    V2 ê·¸ëŒ€ë¡œ ì‚¬ìš©:
    """
    if not title and not summary: return title, summary, keywords
    combined_text = f"Title: {title}\nSummary: {summary}\nKeywords: {keywords}"
    result_text = translate_text(combined_text)
    
    t_title, t_summary, t_keywords = title, summary, keywords
    try:
        lines = result_text.split('\n')
        for line in lines:
            if line.startswith("Title:") or line.startswith("Title :"):
                t_title = line.split(":", 1)[1].strip()
            elif line.startswith("Summary:") or line.startswith("Summary :"):
                t_summary = line.split(":", 1)[1].strip()
            elif line.startswith("Keywords:") or line.startswith("Keywords :"):
                t_keywords = line.split(":", 1)[1].strip()
    except:
        pass
    return t_title, t_summary, t_keywords

# ====================
# Data Loading (V3)
# ====================
def get_weekly_date_range():
    kst = pytz.timezone('Asia/Seoul')
    today = datetime.now(kst)
    yesterday = today - timedelta(days=1)
    last_friday = today - timedelta(days=7)
    return last_friday, yesterday

@st.cache_data(ttl=3600, show_spinner=False)
def load_weekly_data():
    try:
        import glob
        base_dir = "data/articles_raw"
        if not os.path.exists(base_dir):
            base_dir = "../data/articles_raw"
        
        ranked_files = sorted(glob.glob(os.path.join(base_dir, "articles_ranked_*.csv")))
        if not ranked_files:
            return pd.DataFrame(), {}
        
        latest_file = ranked_files[-1]
        df = pd.read_csv(latest_file, encoding='utf-8-sig')
        
        if 'published_date' in df.columns:
            df['published_date'] = pd.to_datetime(df['published_date']).dt.date
        
        if 'category' not in df.columns:
            df['category'] = 'General'
        
        if 'keywords' not in df.columns:
            df['keywords'] = ''
            
        start_date, end_date = get_weekly_date_range()
        info = {
            'start_date': start_date.strftime('%Y-%m-%d'),
            'end_date': end_date.strftime('%Y-%m-%d'),
            'total_articles': len(df),
            'data_file': os.path.basename(latest_file),
            'updated_time': datetime.fromtimestamp(os.path.getmtime(latest_file)).strftime('%Y-%m-%d %H:%M:%S')
        }
        return df, info
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame(), {}

# ====================
# Main UI
# ====================
st.title("ğŸ¥ ZP Market Monitoring - Internal Weekly")
st.caption(f"ë¡œê·¸ì¸: {email} ({access_level})")

df, data_info = load_weekly_data()

if df.empty:
    st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# --- V2 Filter Layout (Top Bar) ---
st.markdown("### ğŸ” Filters & Settings")

f_col1, f_col2, f_col3, f_col4, f_col5, f_col6 = st.columns([1.5, 2, 2, 2, 2, 1.5])

with f_col1:
    lang_opt = st.selectbox("ğŸŒ Language", ["Korean", "English"], index=0)
    use_english = (lang_opt == "English")

with f_col2:
    if 'published_date' in df.columns:
        min_date = df['published_date'].min()
        max_date = df['published_date'].max()
        date_range = st.date_input("ğŸ“… Date Range", [min_date, max_date])
        if len(date_range) == 2:
            start_date, end_date = date_range
        else:
            start_date, end_date = min_date, max_date

with f_col3:
    all_categories = sorted(df['category'].dropna().unique().tolist())
    selected_categories = st.multiselect("ğŸ“‚ Category", all_categories, default=[])
    if not selected_categories: 
        selected_categories = all_categories

# Dynamic Keyword Filter Logic
temp_mask = (
    (df['published_date'] >= start_date) & 
    (df['published_date'] <= end_date) &
    (df['category'].isin(selected_categories))
)
df_filtered_step1 = df[temp_mask]

with f_col4:
    available_keywords = []
    if 'keywords' in df_filtered_step1.columns:
        # í‚¤ì›Œë“œê°€ ì‰¼í‘œ ë“±ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, V2 ë¡œì§ ë‹¨ìˆœí™”
        available_keywords = sorted(df_filtered_step1['keywords'].astype(str).unique().tolist())
    
    # Translate options
    if use_english:
        keyword_options = [KEYWORD_MAPPING.get(k, k) for k in available_keywords]
        en_to_kr = {KEYWORD_MAPPING.get(k, k): k for k in available_keywords}
    else:
        keyword_options = available_keywords
    
    selected_keywords_display = st.multiselect("ğŸ”‘ Keyword", keyword_options, default=[])
    
    if use_english:
        selected_keywords = [en_to_kr.get(k, k) for k in selected_keywords_display]
    else:
        selected_keywords = selected_keywords_display

with f_col5:
    sort_opts = ["AI Relevance", "Latest Date", "Category", "Keyword"]
    sort_mode = st.selectbox("ğŸ“Š Sort By", sort_opts)

with f_col6:
    show_ai_only = st.checkbox("ğŸ¤– AI Only", value=True, help="Show only AI recommended articles")

# --- Apply Final Filter ---
mask = temp_mask
if selected_keywords:
    mask = mask & (df['keywords'].isin(selected_keywords))

if show_ai_only and 'lgbm_score' in df.columns:
    VIP_KEYWORDS = [
        'DKSH', 'GSK', 'MSD', 'ê³µë™íŒë§¤', 'ë…¸ë°”í‹°ìŠ¤', 'ë…¸ë³´ë…¸ë””ìŠ¤í¬',
        'ë¼ë¯¸ì‹¤', 'ë¡œìŠˆ', 'ë¦´ë¦¬', 'ë¸”ë£¨ì— í…', 'ì‚¬ë…¸í”¼', 'ì•”ì  ', 'ì˜¤ê°€ë…¼',
        'ìœ„ê³ ë¹„', 'ì¥´ë¦­', 'ì§€ì˜¤ì˜', 'ì½”í”„ë¡œëª¨ì…˜', 'íŠ¹í—ˆë§Œë£Œ', 'í•œë…', 'í™”ì´ì',
        'ë©”ë‚˜ë¦¬ë‹ˆ'
    ]
    df_temp = df[mask]
    
    # 1. AI Score Filter (Score >= 0.18)
    # V2 used 0.18, let's stick to it or similar.
    # Note: V3 rank_articles produces lgbm_score (0-1 approx).
    ai_candidates = df_temp[df_temp['lgbm_score'] >= 0.18]
    top_ai = ai_candidates.nlargest(20, 'lgbm_score')
    
    # 2. VIP Keyword Filter (Keyword + Score >= 0.01)
    vip_pattern = '|'.join(VIP_KEYWORDS)
    has_vip = df_temp[
        df_temp['title'].str.contains(vip_pattern, case=False, na=False) |
        df_temp['summary'].fillna('').str.contains(vip_pattern, case=False, na=False)
    ]
    vip_candidates = has_vip[has_vip['lgbm_score'] >= 0.01]
    top_vip = vip_candidates.nlargest(5, 'lgbm_score')
    
    filtered_df = pd.concat([top_ai, top_vip]).drop_duplicates(subset=['url'])
else:
    filtered_df = df[mask]

# Sorting
if sort_mode == "AI Relevance":
    # Prefer final_score if exists, else lgbm_score, else score_ag
    if 'final_score' in df.columns:
        filtered_df = filtered_df.sort_values('final_score', ascending=False)
    elif 'lgbm_score' in df.columns:
        filtered_df = filtered_df.sort_values('lgbm_score', ascending=False)
    elif 'score_ag' in df.columns:
        filtered_df = filtered_df.sort_values('score_ag', ascending=False)
elif sort_mode == "Category":
    filtered_df = filtered_df.sort_values('category', ascending=True)
elif sort_mode == "Keyword":
     if 'keywords' in df.columns:
        filtered_df = filtered_df.sort_values('keywords', ascending=True)
else:
    filtered_df = filtered_df.sort_values('published_date', ascending=False)

# Metrics
st.markdown(f"**Total Articles:** {len(filtered_df)}")
st.divider()

# --- Article List Display (Card Style) ---
if filtered_df.empty:
    st.info("No articles found.")
else:
    category_priority = ['Distribution', 'BD', 'Client', 'Zuellig']
    all_categories = filtered_df['category'].unique()
    sorted_categories = [cat for cat in category_priority if cat in all_categories]
    sorted_categories += sorted([cat for cat in all_categories if cat not in category_priority])
    
    for category_name in sorted_categories:
        category_df = filtered_df[filtered_df['category'] == category_name]
        display_category = translate_text(category_name) if use_english else category_name
        
        st.markdown(f"""
        <div style="margin-top: 20px; margin-bottom: 15px;">
            <h3 style="font-size: 22px; color: #006666; border-bottom: 2px solid #0ABAB5; padding-bottom: 8px;">
                ğŸ“‚ {display_category} <span style="color: #888; font-size: 18px;">({len(category_df)} articles)</span>
            </h3>
        </div>
        """, unsafe_allow_html=True)
        
        for _, row in category_df.iterrows():
            title = row['title']
            summary_text = row.get('summary', '')
            category = row['category']
            date = row.get('published_date', '')
            keywords = row.get('keywords', '')
            
            if use_english:
                title, summary_text, keywords_trans = translate_article_batch(title, summary_text, keywords)
                keywords = keywords_trans # Update processed keywords
            
            st.markdown(f"""
            <div class="article-card">
                <div style="font-size: 16px; line-height: 1.5; color: #333;">
                    <a href="{row['url']}" target="_blank" style="font-size: 18px; font-weight: bold; text-decoration: none; color: #008080;">{title}</a>
                    <span style="color: #666;"> | {date} | {keywords}</span>
                </div>
                <div style="font-size: 16px; margin-top: 8px; color: #555; line-height: 1.6;">
                    {summary_text}
                </div>
            </div>
            """, unsafe_allow_html=True)

# --- KakaoTalk Summary Generator (Sidebar) ---
with st.sidebar:
    st.divider()
    st.subheader("ğŸ’¬ Kakao Update")
    if st.button("ğŸ“ Create Summary"):
        with st.spinner("Selecting best articles & formatting..."):
            k_df = filtered_df.copy()
            COMPETITORS = ["ì§€ì˜¤ì˜", "DKSH", "ë¸”ë£¨ì— í…", "ë°”ë¡œíŒœ", "ìš©ë§ˆ", "ì‰¥ì»¤", "DHL", "LXíŒí† ìŠ¤", "CJ"]
            def has_competitor(text):
                return any(comp in str(text) for comp in COMPETITORS)
            k_df = k_df[~k_df['title'].apply(has_competitor)]
            
            # Sort likely column
            sort_c = 'final_score' if 'final_score' in k_df.columns else ('lgbm_score' if 'lgbm_score' in k_df.columns else 'published_date')
            k_df = k_df.sort_values(sort_c, ascending=False).head(20)
            
            # Split
            NEGATIVE_KEYWORDS = ["ê³¼ì§•ê¸ˆ", "í–‰ì •ì²˜ë¶„", "ì ë°œ", "ìœ„ë°˜", "ê²€ì°°", "ì†Œì†¡", "ë¶ˆë§Œ", "ë§¤ê°", "ì² ìˆ˜"]
            def is_distribution_article(row):
                category = row.get('category', '')
                text = str(row['title']) + " " + str(row.get('summary', ''))
                if category == 'Distribution': return True
                if category == 'Supply Issues': return True
                if category == 'Zuellig':
                    if not any(neg in text for neg in NEGATIVE_KEYWORDS): return True
                return False
            
            dist_df = k_df[k_df.apply(is_distribution_article, axis=1)].head(10)
            ind_df = k_df[~k_df.apply(is_distribution_article, axis=1)].head(10)
            
            header_dist = "ğŸ“¦ [ì˜ì•½í’ˆ ìœ í†µ (Distribution)]"
            header_ind = "ğŸ¢ [ì œì•½ ì—…ê³„ (Pharma Industry)]"
            msg_none = "- (ê´€ë ¨ ì£¼ìš” ê¸°ì‚¬ ì—†ìŒ)"
            
            if use_english:
                header_dist = "ğŸ“¦ [Distribution News]"
                header_ind = "ğŸ¢ [Pharma Industry News]"
                msg_none = "- (No major articles found)"

            kakao_msg = f"[ZP Market Monitoring Weekly Update]\nğŸ“… Period: {start_date} ~ {end_date}\n\n"
            
            # Helper to format block
            def format_block(df_block):
                msg = ""
                if df_block.empty:
                    msg += f"{msg_none}\n"
                else:
                    for _, row in df_block.iterrows():
                        t = row['title']
                        s = row.get('summary', '')
                        k = row.get('keywords', '')
                        d = row.get('published_date', '')
                        if use_english:
                            t, s, _ = translate_article_batch(t, s, k)
                        msg += f"{t} | {d}\n{s}\n{row['url']}\n\n"
                return msg

            kakao_msg += f"{header_dist}\n" + format_block(dist_df)
            kakao_msg += f"\n{header_ind}\n" + format_block(ind_df)
            
            if use_english:
                kakao_msg += "\n\nâ„¹ï¸ Note: AI-generated summary."
            else:
                kakao_msg += "\n\nâ„¹ï¸ ì•Œë¦¼: AI ëª¨ë¸ ìë™ ìƒì„± ìš”ì•½ì…ë‹ˆë‹¤."
                
            st.success("âœ… Summary Generated!")
            st.code(kakao_msg, language=None)
