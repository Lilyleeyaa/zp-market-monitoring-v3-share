"""
Internal Weekly Dashboard - ë‚´ë¶€ìš© (ê²½ìŸì‚¬ í¬í•¨)
V2 Design & Filter Logic Restoration (Exact Replica + Feb 06 Fix)
"""
import streamlit as st
import pandas as pd
import sys
import os
import requests
import json
import time
from datetime import datetime, timedelta
import pytz

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from auth.simple_auth import authenticate

# Page configuration
st.set_page_config(
    page_title="Health Market Monitor",
    page_icon="ğŸ¥",
    layout="wide"
)

# Apply Noto Sans KR font globally (V2 Style)
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');

* {
    font-family: 'Noto Sans KR', sans-serif !important;
}

html, body, div, span, p, h1, h2, h3, h4, h5, h6 {
    font-family: 'Noto Sans KR', sans-serif !important;
}

.stMarkdown, .stText, .stButton button, .stSelectbox, .stMultiSelect {
    font-family: 'Noto Sans KR', sans-serif !important;
}

/* Apply to text areas and inputs */
textarea, input, .stTextArea textarea, .stTextInput input {
    font-family: 'Noto Sans KR', sans-serif !important;
}

/* Streamlit specific */
[data-testid="stMarkdownContainer"] {
    font-family: 'Noto Sans KR', sans-serif !important;
}
</style>
""", unsafe_allow_html=True)

# Title (V2 Style)
st.title("ğŸ¥ Healthcare Market Monitoring")
st.markdown("Automated news monitoring & analysis system")

# ì¸ì¦ (ë‚´ë¶€ ì „ìš©)
email, access_level = authenticate(mode='weekly')

if access_level != 'internal':
    st.error("âŒ ì´ ëŒ€ì‹œë³´ë“œëŠ” ë‚´ë¶€ ì‚¬ìš©ìë§Œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    st.stop()
    
# Add version toast to confirm update
st.toast("Updated Code Loaded (v3.0.5)", icon="âœ…")

# ====================
# Translation Components (V2)
# ====================
EXTRA_GLOSSARY = {
    "ë°ì¼ë¦¬íŒœ": "Daily Pharm",
    "ì•½ì‚¬ê³µë¡ ": "Yaksagongron",
    "ë©”ë””íŒŒë‚˜": "Medipana",
    "ì˜í•™ì‹ ë¬¸": "Medical Times",
    "ì²­ë…„ì˜ì‚¬": "Doctor's News",
    "ë‰´ìŠ¤1": "News1",
    "ë‰´ì‹œìŠ¤": "Newsis",
    "ì²˜ë°©ê¶Œ ì§„ì…": "Entry into Prescription Market",
    "ì²˜ë°©ê¶Œ": "Prescription Market",
    "ê¸‰ì—¬ í™•ëŒ€": "Reimbursement Expansion",
    "ê¸‰ì—¬": "Reimbursement",
    "ë¹„ê¸‰ì—¬": "Non-Reimbursement",
    "ì•½ê°€ ì¸í•˜": "Price Cut",
    "ì•½ê°€": "Drug Price",
    "ì œë„¤ë¦­": "Generic",
    "ì˜¤ë¦¬ì§€ë„": "Original",
    "í’ˆì ˆ": "Out of Stock",
    "ê³µê¸‰ë¶€ì¡±": "Supply Shortage",
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption",
    "ì„ìƒ": "Clinical Trial",
    "í—ˆê°€": "Approval",
    "ì‹ì•½ì²˜": "MFDS",
    "ì‹¬í‰ì›": "HIRA",
    "ê±´ë³´ê³µë‹¨": "NHIS",
    "ì•±ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ì—¡ê¸€ë¦¬ìŠ¤": "Ebglyss",
    "ìƒê¸‰ì¢…í•©ë³‘ì›": "Tertiary General Hospital",
    "ê±´ê¸°ì‹": "Health Functional Food",
    "í”„ë¦¬í•„ë“œ": "Pre-filled",
}

KEYWORD_MAPPING = {
    "ì˜ì•½í’ˆìœ í†µ": "Pharmaceutical Distribution", "ì§€ì˜¤ì˜": "GeoYoung", "DKSH": "DKSH", "ë¸”ë£¨ì— í…": "BlueMtech", "ë°”ë¡œíŒœ": "Baropharm", "ìš©ë§ˆ": "Yongma", "ì‰¥ì»¤": "Schenker", "DHL": "DHL", "LXíŒí† ìŠ¤": "LX Pantos", "CJ": "CJ",
    "ê³µë™íŒë§¤": "Co-Promotion", "ì½”í”„ë¡œëª¨ì…˜": "Co-Promotion", "ìœ í†µê³„ì•½": "Distribution Agreement", "íŒê¶Œ": "Sales Rights", "ë¼ì´ì„ ìŠ¤": "License", "M&A": "M&A", "ì¸ìˆ˜": "Acquisition", "í•©ë³‘": "Merger", "ì œíœ´": "Partnership", "íŒŒíŠ¸ë„ˆì‹­": "Partnership", "ê³„ì•½": "Contract", "ìƒë¬¼í•™ì ì œì œ": "Biologics", "ì½œë“œì²´ì¸": "Cold Chain", "CSO": "CSO", "íŒì´‰ì˜ì—…ì": "Sales Agent", "íŠ¹í—ˆë§Œë£Œ": "Patent Expiry", "êµ­ê°€ë°±ì‹ ": "National Vaccine", "ë°±ì‹ ": "Vaccine",
    "í—ˆê°€": "Approval", "ì‹ ì œí’ˆ": "New Product", "ì¶œì‹œ": "Launch", "ì‹ ì•½": "New Drug", "ì ì‘ì¦": "Indication", "ì œí˜•": "Formulation", "ìš©ëŸ‰": "Dosage",
    "ë³´í—˜ë“±ì¬": "Reimbursement", "ê¸‰ì—¬": "NHI Coverage", "ì•½ê°€": "Drug Price",
    "ì¥´ë¦­": "Zuellig", "ì§€í”¼í…Œë¼í“¨í‹±ìŠ¤": "ZP Therapeutics", "ë¼ë¯¸ì‹¤": "Lamisil", "ì•¡í‹°ë„˜": "Actinum", "ë² íƒ€ë”˜": "Betadine", "ì‚¬ì´í´ë¡œì œìŠ¤íŠ¸": "Cyclogest", "ë¦¬ë¸Œíƒ€ìš”": "Libtayo",
    "í•œë…": "Handok", "MSD": "MSD", "ì˜¤ê°€ë…¼": "Organon", "í™”ì´ì": "Pfizer", "ì‚¬ë…¸í”¼": "Sanofi", "ì•”ì  ": "Amgen", "GSK": "GSK", "ë¡œìŠˆ": "Roche", "ë¦´ë¦¬": "Lilly", "ë…¸ë°”í‹°ìŠ¤": "Novartis", "ë…¸ë³´ë…¸ë””ìŠ¤í¬": "Novo Nordisk", "ë¨¸í¬": "Merck", "ë ˆì½”ë¥´ë‹¤í‹°": "Recordati", "ì…€ì§„": "Celgene", "í…Œë°”í•œë…": "Teva-Handok", "ë² ë§ê±°ì¸ê²”í•˜ì„": "Boehringer Ingelheim", "BMS": "BMS", "ì•„ìŠ¤íŠ¸ë¼ì œë„¤ì¹´": "AstraZeneca", "ì• ë¸Œë¹„": "AbbVie", "íŒŒë§ˆë…¸ë¹„ì•„": "Pharmanovia", "ë¦¬ì œë„¤ë¡ ": "Regeneron", "ë°”ì´ì—˜": "Bayer", "ì•„ìŠ¤í…”ë¼ìŠ¤": "Astellas", "ì–€ì„¼": "Janssen", "ë°”ì´ì˜¤ì  ": "Biogen", "ì…ì„¼": "Ipsen", "ì• ë³´íŠ¸": "Abbott", "ì•ˆí…ì§„": "Antengene", "ë² ì´ì§„": "BeiGene", "ì…€íŠ¸ë¦¬ì˜¨": "Celltrion", "í—¤ì¼ë¦¬ì˜¨": "Haelion", "ì˜¤í ë¼": "Opella", "ì¼„ë·°": "Kenvue", "ë¡œë ˆì•Œ": "L'Oreal", "ë©”ë‚˜ë¦¬ë‹ˆ": "Menarini", "ìœ„ê³ ë¹„": "Wegovy", "ë§ˆìš´ìë¡œ": "Mounjaro",
    "ë‚œì„": "Infertility", "ë¶ˆì„": "Infertility", "í•­ì•”ì œ": "Anticancer",
    "ê³µê¸‰ì¤‘ë‹¨": "Supply Disruption", "ê³µê¸‰ë¶€ì¡±": "Supply Shortage", "í’ˆì ˆ": "Out of Stock", "í’ˆê·€": "Shortage",
}

# ====================
# Filter Logic Definitions (Global)
# ====================
INTERNAL_KEYWORDS = list(KEYWORD_MAPPING.keys())

EXCLUDED_KEYWORDS = [
    "ë„¤ì´ë²„ ë°°ì†¡", "ë„¤ì´ë²„ ì‡¼í•‘", "ë„¤ì´ë²„ í˜ì´", "ë„ì°©ë³´ì¥", 
    "ì¿ íŒ¡", "ë°°ë‹¬ì˜ë¯¼ì¡±", "ìš”ê¸°ìš”", "ë¬´ì‹ ì‚¬", "ì»¬ë¦¬", "ì•Œë¦¬ìµìŠ¤í”„ë ˆìŠ¤", "í…Œë¬´",
    "ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì „ì„¸", "ë§¤ë§¤", "ì²­ì•½", "ê±´ì„¤", 
    "ê¸ˆë¦¬ ì¸í•˜", "ì£¼ì‹ ê°œì¥", "í™˜ìœ¨", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¦ì‹œ", "ìƒí•œê°€", 
    "ì£¼ê°€", "ì£¼ì‹", "ëª©í‘œì£¼ê°€", "íŠ¹ì§•ì£¼", "ê¸‰ë“±",
    "ì—¬í–‰", "í˜¸í…”", "í•­ê³µê¶Œ", "ì˜ˆëŠ¥", "ë“œë¼ë§ˆ", "ì¶•êµ¬", "ì•¼êµ¬", "ì˜¬ë¦¼í”½", "ì—°ì˜ˆ", "ê³µì—°", "ë®¤ì§€ì»¬", "ì „ì‹œíšŒ", "ê´€ëŒ",
    "ì´ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ì „ê¸°ì°¨", "ë°˜ë„ì²´", "ë””ìŠ¤í”Œë ˆì´", "ì¡°ì„ ", "ì² ê°•",
    "ì±„ìš©", "ì‹ ì…ì‚¬ì›", "ê³µì±„"
]

GENERIC_KEYWORDS = ["ê³„ì•½", "M&A", "ì¸ìˆ˜", "í•©ë³‘", "íˆ¬ì", "ì œíœ´", "CJ"]
PHARMA_CONTEXT_KEYWORDS = ["ì œì•½", "ë°”ì´ì˜¤", "ì‹ ì•½", "ì„ìƒ", "í—¬ìŠ¤ì¼€ì–´", "ì˜ë£Œ", "ë³‘ì›", "ì•½êµ­", "ì¹˜ë£Œì œ", "ë°±ì‹ ", "ì§„ë‹¨", "ë¬¼ë¥˜", "ìœ í†µ", "ê³µê¸‰"]

def is_noise_article(row):
    text = str(row['title']) + " " + str(row.get('summary', ''))
    
    # 1. Check Explicit Exclusions
    for exc in EXCLUDED_KEYWORDS:
        if exc in text:
            return True
            
    # 2. Homonym Check: "ì œì•½" (Constraint vs Pharma)
    if "ì œì•½" in text:
        if any(x in text for x in ["ì‹œê°„ ì œì•½", "ê³µê°„ ì œì•½", "ë¬¼ë¦¬ì  ì œì•½", "ë°œì „ ì œì•½", "í™œë™ ì œì•½"]):
            if not any(pk in text for pk in PHARMA_CONTEXT_KEYWORDS if pk != "ì œì•½"):
                return True

    # 3. Generic Keyword Context Check
    row_kws = str(row.get('keywords', ''))
    if row_kws:
        matched_gen = [gk for gk in GENERIC_KEYWORDS if gk in row_kws]
        if matched_gen:
             if not any(pk in text for pk in PHARMA_CONTEXT_KEYWORDS):
                 return True
    return False

def has_internal_keyword(row_keywords):
    if pd.isna(row_keywords) or row_keywords == '':
        return False
    row_k_list = str(row_keywords).split(',') 
    for k in row_k_list:
        if k.strip() in INTERNAL_KEYWORDS:
            return True
    return False

GENAI_API_KEY = "AIzaSyD5HUixHFDEeifmY5NhJCnL4cLlxOp7fp0"
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GENAI_API_KEY}"

@st.cache_data(show_spinner=False)
def translate_text(text, target='en'):
    if not text: return ""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
            glossary_context = "\n".join([f"- {k}: {v}" for k, v in full_glossary.items()])
            
            prompt = f"""
            You are a professional pharmaceutical translator. 
            Translate the following Korean text to English.
            
            Rules:
            1. Maintain professional industry terminology.
            2. Use the specific glossary below for strict term matching:
            {glossary_context}
            
            Text to translate:
            "{text}"
            
            Output only the translated English text, no explanations.
            """
            
            payload = {"contents": [{"parts": [{"text": prompt}]}]}
            headers = {'Content-Type': 'application/json'}
            response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload), timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    return result['candidates'][0]['content']['parts'][0]['text'].strip()
            elif response.status_code == 429:
                time.sleep(2)
                continue
            else:
                break
        except Exception as e:
            break
            
    try:
        from deep_translator import GoogleTranslator
        processed_text = text
        full_glossary = {**KEYWORD_MAPPING, **EXTRA_GLOSSARY}
        sorted_terms = sorted(full_glossary.keys(), key=len, reverse=True)
        for kr_term in sorted_terms:
            if kr_term in processed_text:
                processed_text = processed_text.replace(kr_term, full_glossary[kr_term])
        return GoogleTranslator(source='ko', target=target).translate(processed_text)
    except:
        return text

@st.cache_data(show_spinner=False)
def translate_article_batch(title, summary, keywords):
    if not title and not summary: return title, summary, keywords
    combined_text = f"Title: {title}\nSummary: {summary}\nKeywords: {keywords}"
    result_text = translate_text(combined_text)
    
    t_title, t_summary, t_keywords = title, summary, keywords
    try:
        lines = result_text.split('\n')
        for line in lines:
            if line.startswith("Title:") or line.startswith("Title :"):
                t_title = line.split(":", 1)[1].strip()
            elif line.startswith("Summary:") or line.startswith("Summary :"):
                t_summary = line.split(":", 1)[1].strip()
            elif line.startswith("Keywords:") or line.startswith("Keywords :"):
                t_keywords = line.split(":", 1)[1].strip()
    except:
        pass
    return t_title, t_summary, t_keywords

# ====================
# Data Loading (V3 Logic with Strict Filter)
# ====================
def get_weekly_date_range():
    # 2ì›” 6ì¼ ê¸°ì¤€ ì£¼ê°„ (User Request)
    target_date = datetime(2026, 2, 6).date()
    start_date = target_date - timedelta(days=7) # ~7 days range
    return start_date, target_date

@st.cache_data(ttl=3600, show_spinner=False)
def load_weekly_data():
    try:
        import glob
        base_dir = "data/articles_raw"
        if not os.path.exists(base_dir):
            base_dir = "../data/articles_raw"
        
        ranked_files = sorted(glob.glob(os.path.join(base_dir, "articles_ranked_*.csv")))
        if not ranked_files:
            return pd.DataFrame(), {}, "No Files"
        
        latest_file = ranked_files[-1]
        df = pd.read_csv(latest_file, encoding='utf-8-sig') 
        
        if 'published_date' in df.columns:
            df['published_date'] = pd.to_datetime(df['published_date']).dt.date
        
        if 'category' not in df.columns:
            df['category'] = 'General'
        
        if 'keywords' not in df.columns:
            df['keywords'] = ''
            
        # --- Apply Cached Filters Here (Performance Optimization) ---
        # 1. Internal Keyword Strict Filter
        df['has_internal_kw'] = df['keywords'].apply(has_internal_keyword)
        df = df[df['has_internal_kw']]
        
        # 2. Noise Filter
        if not df.empty:
            df['is_noise'] = df.apply(is_noise_article, axis=1)
            df = df[~df['is_noise']]
            
        return df, os.path.basename(latest_file), "AI Ranked"
    except Exception as e:
        return pd.DataFrame(), None, str(e)

df, filename, file_type = load_weekly_data()

if filename:
    if file_type == "AI Ranked":
         st.toast(f"Loaded: {filename} (AI Ranked)", icon="ğŸ¤–")
    else:
         st.toast(f"Loaded: {filename} (Raw Data)", icon="ğŸ“‚")
elif file_type and "None" not in str(file_type): 
    st.error(f"Error loading data: {file_type}")

if df.empty:
    st.warning("No data found. Please run the crawler first.")
    st.stop()

# Internal Keyword Strict Filter & Noise Filter applied inside load_weekly_data for caching


# ====================
# Main Layout (V2 Style)
# ====================
st.markdown("""
<style>
    /* Global Background & Font */
    .stApp {
        background-color: #F0F8F8; /* Very Light Teal/Grey */
    }
    
    /* Header/Title */
    h1 {
        color: #006666 !important; /* Deep Teal */
    }
    
    /* Article Card Styles */
    .article-card {
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 15px;
        background-color: #ffffff; /* White card */
        border-left: 5px solid #0ABAB5; /* Tiffany Blue Accent */
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    .article-title {
        font-size: 18px;
        font-weight: bold;
        color: #008080; /* Teal */
        text-decoration: none;
    }
    .article-title:hover {
        color: #0ABAB5; /* Tiffany Blue on Hover */
        text_decoration: underline;
    }
    
    .article-meta {
        font-size: 12px;
        color: #888;
    }
    
    .category-badge {
        background-color: #E0F2F1; /* Light Teal background */
        color: #00695C; /* Dark Teal text */
        padding: 4px 8px;
        border-radius: 12px;
        font-size: 12px;
        font-weight: 500;
        margin-left: 5px;
    }

    .article-summary {
        font-size: 14px;
        color: #444;
        margin-top: 8px;
        line-height: 1.6;
    }
    
    /* Button Styles */
    .stButton>button {
        background-color: #0ABAB5 !important;
        color: white !important;
        border: none;
    }
</style>
""", unsafe_allow_html=True)

# Noise Cleanup Logic moved to global scope and applied in cached loader



# Top Control Bar (Language & Filters)
st.markdown("### ğŸ” Filters & Settings")

f_col1, f_col2, f_col3, f_col4, f_col5, f_col6 = st.columns([1.5, 2, 2, 2, 2, 1.5])

with f_col1:
    lang_opt = st.selectbox("ğŸŒ Language", ["Korean", "English"], index=0)
    use_english = (lang_opt == "English")

with f_col2:
    # Force 2/6 Week as default
    start_week, end_week = get_weekly_date_range()
    
    if 'published_date' in df.columns:
        min_date = df['published_date'].min()
        max_date = df['published_date'].max()
        
        # Override with strict user requirement if available in data
        default_start = max(min_date, start_week) if min_date else start_week
        default_end = min(max_date, end_week) if max_date else end_week

        date_range = st.date_input("ğŸ“… Date Range", [default_start, default_end])
            
        if isinstance(date_range, list) and len(date_range) == 2:
            start_date, end_date = date_range
        else:
            start_date, end_date = default_start, default_end
    else:
        start_date, end_date = None, None

with f_col3:
    all_categories = sorted(df['category'].dropna().unique().tolist())
    selected_categories = st.multiselect("ğŸ“‚ Category", all_categories, default=[])
    if not selected_categories: 
        selected_categories = all_categories

# Dynamic Keyword Filter
temp_mask = pd.Series([True] * len(df))
if start_date and end_date:
    temp_mask = (df['published_date'] >= start_date) & (df['published_date'] <= end_date) & (df['category'].isin(selected_categories))

df_filtered_step1 = df[temp_mask]

with f_col4:
    available_keywords = []
    if 'keywords' in df_filtered_step1.columns:
        available_keywords = sorted(df_filtered_step1['keywords'].astype(str).unique().tolist())
    
    if use_english:
        keyword_options = [KEYWORD_MAPPING.get(k, k) for k in available_keywords]
        en_to_kr = {KEYWORD_MAPPING.get(k, k): k for k in available_keywords}
    else:
        keyword_options = available_keywords
    
    selected_keywords_display = st.multiselect("ğŸ”‘ Keyword", keyword_options, default=[])
    
    if use_english:
        selected_keywords = [en_to_kr.get(k, k) for k in selected_keywords_display]
    else:
        selected_keywords = selected_keywords_display

with f_col5:
    sort_opts = ["AI Relevance", "Latest Date", "Category", "Keyword"]
    sort_mode = st.selectbox("ğŸ“Š Sort By", sort_opts)

with f_col6:
    show_ai_only = st.checkbox("ğŸ¤– AI Only", value=True, help="Show only AI recommended articles")

# --- Apply Final Filter ---
mask = temp_mask
if selected_keywords:
    mask = mask & (df['keywords'].isin(selected_keywords))

if show_ai_only and 'lgbm_score' in df.columns:
    VIP_KEYWORDS = [
        'DKSH', 'GSK', 'MSD', 'ê³µë™íŒë§¤', 'ë…¸ë°”í‹°ìŠ¤', 'ë…¸ë³´ë…¸ë””ìŠ¤í¬',
        'ë¼ë¯¸ì‹¤', 'ë¡œìŠˆ', 'ë¦´ë¦¬', 'ë¸”ë£¨ì— í…', 'ì‚¬ë…¸í”¼', 'ì•”ì  ', 'ì˜¤ê°€ë…¼',
        'ìœ„ê³ ë¹„', 'ì¥´ë¦­', 'ì§€ì˜¤ì˜', 'ì½”í”„ë¡œëª¨ì…˜', 'íŠ¹í—ˆë§Œë£Œ', 'í•œë…', 'í™”ì´ì',
        'ë©”ë‚˜ë¦¬ë‹ˆ'
    ]
    df_temp = df[mask]
    
    score_col = 'final_score' if 'final_score' in df_temp.columns else 'lgbm_score'
    
    # 1. AI Top 20
    ai_threshold = 0.18
    ai_candidates = df_temp[df_temp[score_col] >= ai_threshold]
    top_ai = ai_candidates.nlargest(20, score_col)
    
    # 2. VIP Top 5 (Safety net)
    vip_pattern = '|'.join(VIP_KEYWORDS)
    has_vip = df_temp[
        df_temp['title'].str.contains(vip_pattern, case=False, na=False) |
        df_temp['summary'].fillna('').str.contains(vip_pattern, case=False, na=False)
    ]
    vip_threshold = 0.01
    vip_candidates = has_vip[has_vip[score_col] >= vip_threshold]
    top_vip = vip_candidates.nlargest(5, score_col)
    
    filtered_df = pd.concat([top_ai, top_vip]).drop_duplicates(subset=['url'])
else:
    filtered_df = df[mask]

# Sorting
if sort_mode == "AI Relevance":
    if 'final_score' in df.columns:
        filtered_df = filtered_df.sort_values('final_score', ascending=False)
    elif 'lgbm_score' in df.columns:
        filtered_df = filtered_df.sort_values('lgbm_score', ascending=False)
    elif 'score_ag' in df.columns:
        filtered_df = filtered_df.sort_values('score_ag', ascending=False)
elif sort_mode == "Category":
    filtered_df = filtered_df.sort_values('category', ascending=True)
elif sort_mode == "Keyword":
     if 'keywords' in df.columns:
        filtered_df = filtered_df.sort_values('keywords', ascending=True)
else:
    filtered_df = filtered_df.sort_values('published_date', ascending=False)

# Metrics
st.markdown(f"**Total Articles:** {len(filtered_df)}")
st.divider()

# --- Article List Display (Card Style) ---
if filtered_df.empty:
    st.info("No articles found.")
else:
    category_priority = ['Distribution', 'BD', 'Client', 'Zuellig']
    all_categories = filtered_df['category'].unique()
    sorted_categories = [cat for cat in category_priority if cat in all_categories]
    sorted_categories += sorted([cat for cat in all_categories if cat not in category_priority])
    
    for category_name in sorted_categories:
        category_df = filtered_df[filtered_df['category'] == category_name]
        display_category = translate_text(category_name) if use_english else category_name
        
        st.markdown(f'''
        <div style="margin-top: 20px; margin-bottom: 15px;">
            <h3 style="font-size: 22px; color: #006666; border-bottom: 2px solid #0ABAB5; padding-bottom: 8px;">
                {display_category} <span style="color: #888; font-size: 18px;">({len(category_df)} articles)</span>
            </h3>
        </div>
        ''', unsafe_allow_html=True)
        
        for _, row in category_df.iterrows():
            title = row['title']
            summary_text = row.get('summary', '')
            category = row['category']
            date = row.get('published_date', '')
            keywords = row.get('keywords', '')
            
            if use_english:
                title, summary_text, keywords_trans = translate_article_batch(title, summary_text, keywords)
                keywords = keywords_trans
            
            st.markdown(f'''
            <div class="article-card">
                <div style="font-size: 16px; line-height: 1.5; color: #333;">
                    <a href="{row['url']}" target="_blank" style="font-size: 18px; font-weight: bold; text-decoration: none; color: #008080;">{title}</a>
                    <span style="color: #666;"> | {date} | {keywords}</span>
                </div>
                <div style="font-size: 16px; margin-top: 8px; color: #555; line-height: 1.6;">
                    {summary_text}
                </div>
            </div>
            ''', unsafe_allow_html=True)

# --- KakaoTalk Summary Generator (Sidebar) ---
with st.sidebar:
    st.divider()
    st.subheader("ğŸ’¬ Kakao Update")
    if st.button("ğŸ“ Create Summary"):
        with st.spinner("Selecting best articles & formatting..."):
            k_df = filtered_df.copy()
            COMPETITORS = ["ì§€ì˜¤ì˜", "DKSH", "ë¸”ë£¨ì— í…", "ë°”ë¡œíŒœ", "ìš©ë§ˆ", "ì‰¥ì»¤", "DHL", "LXíŒí† ìŠ¤", "CJ"]
            def has_competitor(text):
                return any(comp in str(text) for comp in COMPETITORS)
            k_df = k_df[~k_df['title'].apply(has_competitor)]
            
            sort_c = 'final_score' if 'final_score' in k_df.columns else ('lgbm_score' if 'lgbm_score' in k_df.columns else 'published_date')
            k_df = k_df.sort_values(sort_c, ascending=False).head(20)
            
            NEGATIVE_KEYWORDS = ["ê³¼ì§•ê¸ˆ", "í–‰ì •ì²˜ë¶„", "ì ë°œ", "ìœ„ë°˜", "ê²€ì°°", "ì†Œì†¡", "ë¶ˆë§Œ", "ë§¤ê°", "ì² ìˆ˜"]
            def is_distribution_article(row):
                category = row.get('category', '')
                text = str(row['title']) + " " + str(row.get('summary', ''))
                if category == 'Distribution': return True
                if category == 'Supply Issues': return True
                if category == 'Zuellig':
                    if not any(neg in text for neg in NEGATIVE_KEYWORDS): return True
                return False
            
            dist_df = k_df[k_df.apply(is_distribution_article, axis=1)].head(10)
            ind_df = k_df[~k_df.apply(is_distribution_article, axis=1)].head(10)
            
            header_dist = "ğŸ“¦ [ì˜ì•½í’ˆ ìœ í†µ (Distribution)]"
            header_ind = "ğŸ¢ [ì œì•½ ì—…ê³„ (Pharma Industry)]"
            msg_none = "- (ê´€ë ¨ ì£¼ìš” ê¸°ì‚¬ ì—†ìŒ)"
            
            if use_english:
                header_dist = "ğŸ“¦ [Distribution News]"
                header_ind = "ğŸ¢ [Pharma Industry News]"
                msg_none = "- (No major articles found)"

            kakao_msg = f"[ZP Market Monitoring Weekly Update]\nğŸ“… Period: {start_date} ~ {end_date}\n\n"
            
            def format_block(df_block):
                msg = ""
                if df_block.empty:
                    msg += f"{msg_none}\n"
                else:
                    for _, row in df_block.iterrows():
                        t = row['title']
                        s = row.get('summary', '')
                        k = row.get('keywords', '')
                        d = row.get('published_date', '')
                        if use_english:
                            t, s, _ = translate_article_batch(t, s, k)
                        msg += f"{t} | {d}\n{s}\n{row['url']}\n\n"
                return msg

            kakao_msg += f"{header_dist}\n" + format_block(dist_df)
            kakao_msg += f"\n{header_ind}\n" + format_block(ind_df)
            
            if use_english:
                kakao_msg += "\n\nâ„¹ï¸ Note: AI-generated summary."
            else:
                kakao_msg += "\n\nâ„¹ï¸ ì•Œë¦¼: AI ëª¨ë¸ ìë™ ìƒì„± ìš”ì•½ì…ë‹ˆë‹¤."
                
            st.success("âœ… Summary Generated!")
            st.code(kakao_msg, language=None)
